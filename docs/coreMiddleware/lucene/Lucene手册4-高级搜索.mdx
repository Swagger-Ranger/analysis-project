---
title: Lucene手册4-高级搜索
sidebar_position: 4
toc_min_heading_level: 2
toc_max_heading_level: 5
---
## lucene内置高级搜索特性

### 域缓存（field cache）

要能够很快地访问各个文档中某个域的值，Lucene 的倒排索引是不支持这个的，因为它已经为了快速访问所有文档中所包含的特定项而做了优化处理。被存储的域和项向量使你可以通过文档号来访问所有的域值，但它们的加载速度比较慢，并且通常不能同时生成超过一页的搜索结果。Lucene 的域缓存是一个高级的内部API，是一项用户不可见的搜索功能；并且它在某种程度上是一个构建模块，一个实用的内部API，创建它的目的就是为了满足以上需求。

通常你的应用程序不会直接使用域缓存，但高级搜索功能需要使用它，如根据域值对搜索结果进行排序，以及在后台使用域缓存等。除了排序功能，Lucene 的一些内置过滤器和一些功能查询都会在内部使用域缓存，因此重要的是理解这其中的性能取舍。

> Lucene 4.0及以后的版本中，`FieldCache`类已经被废弃，并被`DocValues`所取代。`DocValues`提供了一种更高效的列式存储机制，用于存储字段值并支持高效的排序和聚合操作。在Elasticsearch中，`FieldCache`的功能主要由`DocValues`和`Fielddata`实现。
>
> `text`类型不使用`docValues`,对应的是`Fielddata`，而且默认不开启。对于需要排序或聚合的文本字段，可以启用`Fielddata`。但`Fielddata`会消耗大量内存，因此应谨慎使用。

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/lucene/lucene-28.png" alt="image" style={{ maxWidth: '60%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>


**为什么废弃FieldCache**

1. **性能问题**：`FieldCache`在使用时会将所有需要的字段数据加载到内存中，这可能导致内存消耗过大，尤其是当索引包含大量文档和大字段时。在搜索请求中首次使用`FieldCache`时，可能会引起较长时间的延迟，因为它需要将数据从磁盘加载到内存中。
2. **垃圾回收问题**：`FieldCache`的生命周期与索引读者（IndexReader）绑定，因此每次索引读者重新打开时，`FieldCache`需要重新加载。这会导致频繁的垃圾回收（GC）问题，影响系统的稳定性和性能。
3. **缺乏灵活性**：`FieldCache`只能在内存中存储和访问字段值，不适合存储较大的数据集或需要频繁更新的数据集。并且`FieldCache`的API较为底层，不易于扩展和定制。

**DocValues的改进**

1. **磁盘上的列式存储**：`DocValues`将数据按列存储在磁盘上，只有在需要时才加载到内存中，这减少了内存使用和首次加载的延迟。

并且列式存储使得数据压缩和读取更加高效，尤其是在执行聚合和排序操作时。

2. **缓存友好**：`DocValues`使用操作系统的文件系统缓存（OS cache），在多次查询时可以更好地利用缓存，从而提高性能。由于数据按列存储，读取特定字段的数据时不需要加载整个文档的数据，进一步优化了性能。

3. **灵活性和扩展性**：`DocValues`支持多种数据类型，包括数值型、字符串型和二进制型，且易于扩展。支持单值和多值字段，更好地满足各种复杂的数据存储需求。

4. **一致性和可靠性**：`DocValues`的数据存储格式经过优化和压缩，提供更高的读取效率和一致性。与索引数据一起存储和管理，减少了数据不一致的风险。



#### DocValues

**DocValues，其实就是正排索引，以供排序，聚合，过滤等操作使用**，并且es中使用DocValues的功能来对快速访问各个文档中某个域的值这种需求的支持。并且使用非常方便。

- **DocValues支持的类型**

DocValues支持多种字段类型，包括数值类型（整数、浮点数）、布尔类型、日期类型以及关键词（keyword）类型。特别地，对于包含集合内容的字段，可以使用多值字段（multi-valued fields）,因为DocValues 会为每个文档的这个字段存储一个列表，而不是单个值。它们在索引时被构建，并在搜索时被加载到内存中，以提高读取和操作性能。*字段类型（如`keyword`、`numeric`、`date`等）会默认启用DocValues*。

- **DocValues数据存储**

DocValues在Elasticsearch和Lucene中在索引时被构建，作为单独存储的数据列。它们将文档字段的值按列存储在磁盘上，而不是按行存储。这种设计有助于提高检索、排序和聚合操作的性能。

- **DocValues示例**

```json
PUT /my_index
{
  "mappings": {
    "properties": {
      "name": {
        "type": "keyword"
      },
      "age": {
        "type": "integer"
      },
      "tags": {
        "type": "keyword"
      }
    }
  }
}
  ```

```json
PUT /my_index/_doc/1
{
  "name": "Alice",
  "age": 30,
  "tags": ["developer", "java"]
}

PUT /my_index/_doc/2
{
  "name": "Bob",
  "age": 25,
  "tags": ["developer", "python"]
}
  ```

**获取docValues**

```java
public class DocValuesExample {
    public static void main(String[] args) throws IOException {
        DirectoryReader reader = DirectoryReader.open(directory);
        // 获取数值
        for (LeafReaderContext context : reader.leaves()) {
            LeafReader leafReader = context.reader();
            NumericDocValues docValues = DocValues.getNumeric(leafReader, "field_name");
            Bits liveDocs = leafReader.getLiveDocs();

            for (int i = 0; i < leafReader.maxDoc(); i++) {
                if (liveDocs == null || liveDocs.get(i)) {
                    long value = docValues.get(i);
                    System.out.println("DocID " + i + ": " + value);
                }
            }
        }

				// 获取字符串
        for (LeafReaderContext context : reader.leaves()) {
            LeafReader leafReader = context.reader();
            SortedDocValues sortedDocValues = DocValues.getSorted(leafReader, "sorted_field");
            for (int i = 0; i < leafReader.maxDoc(); i++) {
                if (!sortedDocValues.advanceExact(i)) {
                    continue;
                }
                int ord = sortedDocValues.ordValue();
                String value = sortedDocValues.lookupOrd(ord).utf8ToString();
                System.out.println("DocID " + i + ": " + value);
            }
        }
        reader.close();
        directory.close();
    }
}
```



### 搜索结果排序

在默认情况下，Lucene 通过关联评分对匹配文档进行降序排列，这样就使得关联性最强的文档被排在首位。这种默认排序方式的效果是很好的，因为它会尽可能使得用户能在首先出现的几个搜索结果中找到自己所需要的文档，而不是翻页找寻。然而，你也经常需要用户提供选项以实现各种不同的排序方式。

查询排序的API就是 lucene-core中的`Search`子类中传入`Sort` 排序对象。

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/lucene/lucene-30.png" alt="image" style={{ maxWidth: '70%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>


#### 相关性排序

Lucene按照相关性排序是通过计算每个文档与查询的相关性得分，然后根据得分对结果进行排序的。lucene查询默认就是相关性排序并且，降序排列。`Sort.RELEVANCE`就等于直接new `Sort`对象

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/lucene/lucene-31.png" alt="image" style={{ maxWidth: '70%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>


相关性得分是通过Lucene的评分机制来计算的，得分的值通常是相对的，而不是绝对的，也就是说，一个文档的得分与另一个文档的得分比较有意义，但单独看一个文档的得分并没有绝对意义。并且得分的范围没有固定的上下限。具体得分值取决于许多因素，包括查询的复杂性、文档内容、索引结构等。得分通常是正数，但在某些情况下（例如，某些自定义评分函数）可能会出现负数。

##### 查询评分

Elasticsearch 中，会涉及评分的查询通常包括以下几种类型：

1. **全文查询（Full-text queries）**：`match`，`match_phrase`，`multi_match`，`common_terms`，`query_string`，`simple_query_string`
2. **布尔查询（Boolean queries）**：`bool` 在包含上述查询类型时，布尔查询也会涉及评分，因为bool查询其实就是对查询条件的组合，所以如果bool查询中包含了上述的查询类型，也是会涉及结果评分的。
3. **功能评分查询（Function Score Query）**：`function_score`

这些查询的评分是基于 Lucene 的评分机制，主要包括以下几个因素：

- **词频（TF - Term Frequency）**：词在文档中出现的次数。
- **逆文档频率（IDF - Inverse Document Frequency）**：词在所有文档中出现的频率。
- **字段长度规范化（Field Length Norms）**：字段的长度对评分的影响。
- 规范化主要用于调整因字段长度不同而导致的评分差异。规范化的主要目标是确保在评分时，字段的长度不会对结果产生不合理的影响。
- norm=1/sqrt(field length)，即field length平方根的倒数

**评分公式**

> TF-IDF：score=TF×IDF×field norm
>
> BM25：默认情况下，Lucene 使用 BM25 算法来计算相关性得分，BM25在实践中往往比传统的TF-IDF更有效且计算成本更低
>
> 其中：
>
> - IDF(t) 是词项 t 的逆文档频率
> - f(t,d)是词项 tt 在文档 d中的词频
> - ∣d∣ 是文档 d 的长度
> - avgdl 是所有文档的平均长度
> - K1 和 b 是 BM25 的可调参数

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/lucene/lucene-29.png" alt="image" style={{ maxWidth: '70%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>


##### 评分计算优化

**注意：计算得分往往会带来额外的性能损耗。**

**评估评分操作的资源消耗**

1. 查询复杂度：查询的复杂度会直接影响评分的计算。复杂的查询（例如包含多个条件的布尔查询）通常会消耗更多的资源。
2. 数据量：索引中的文档数量和查询匹配的文档数量都会影响评分的计算开销。较大的索引和匹配更多文档的查询需要更多的计算资源。
3. 字段规范化：如果查询的字段设置了规范化，会增加评分计算的复杂性，因为需要考虑字段长度对评分的影响。
4. 自定义评分函数：如果使用了功能评分查询，自定义的评分函数（例如脚本评分、随机评分等）会增加计算开销。

相关性查询优化

1. **索引优化**

- 索引压缩：使用索引压缩技术（如Block K-编码、For-Delta编码）减少索引文件的大小，从而减少I/O操作的开销。

- 合并小段：定期合并小的索引段（segments），减少段的数量，提高查询效率。

2. **查询优化**

- 预过滤查询：在进行相关性排序前，先使用简单的过滤条件预过滤掉不相关的文档，减少参与相关性计算的文档数量。
- 查询缓存：对常用的查询结果进行缓存，减少重复查询的计算开销。

3. **硬件优化**

- SSD存储：使用SSD存储代替传统的HDD，减少数据读取的延迟。
- 内存缓存：增加内存，尽量将索引数据加载到内存中，减少磁盘I/O。

4. **并行和分布式处理**

- 并行计算：使用多线程或多进程并行计算相关性得分，充分利用多核CPU的计算能力。
- 分布式搜索：在分布式环境中，将索引数据分片（shard）存储到多个节点上，并行处理查询请求，提高查询性能。

5. **算法优化**

- 简化评分模型：使用简化的评分模型，减少计算复杂度。例如，使用BM25替代TF-IDF
- 提前终止评分：在进行相关性排序时，可以设置评分阈值或提前终止条件，对于得分低于一定值的文档直接丢弃，减少不必要的计算。



##### 不涉及评分的排序

一些查询不涉及评分，结果通常会被赋予一个默认得分（通常是1.0）。这些查询往往是过滤查询或布尔查询的一部分，用于筛选数据而不是计算相关性。即便这些查询默认分数是1.0，如果不指定别的排序则查询结果仍然会按默认得分排序返回。

不涉及评分的查询类型：

1. Filter Context Queries：`term query`，`range query`，`exists query`，`prefix query`
2. Constant Score Query：将任意查询包裹在`constant_score`查询中，该查询会忽略原本的相关性得分，返回固定得分。



##### doTrackScores 和 doMaxScore

`Lucent-core`的包中`org.apache.lucene.search.IndexSearcher#search(org.apache.lucene.search.Query, int, org.apache.lucene.search.Sort, boolean, boolean)`查询方法，可以看到两个boolean参数，用以控制结果得分的处理。

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/lucene/lucene-32.png" alt="image" style={{ maxWidth: '70%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>


**doTrackScores **

`doTrackScores` 控制是否在搜索过程中计算和跟踪每个文档的评分（score）。如果设置为 `true`，那么每个匹配的文档都会计算一个相关性评分，这对于按相关性排序或需要展示评分的场景是必需的。在涉及评分的查询时，此参数都默认是开启的。

**相关性查询中doTrackScores为false可能会导致查询结果不准确**

1. 结果无法排序：因为Lucene 在搜索过程中不计算和跟踪每个匹配文档的评分（score），这意味着搜索结果无法按相关性评分进行排序。
2. 排序不一致：在没有评分的情况下，Lucene 会使用其他默认排序标准（例如文档ID）来排序结果。这样，返回的文档顺序可能与预期的相关性排序不一致。
3. 局部排序错误：如果某些部分的查询过程中需要评分（例如分页时），但评分信息不可用，那么分页结果可能仅按局部排序（如当前页的默认排序）而不是全局相关性排序。这可能导致返回的结果顺序在分页时不一致。

**doMaxScore**

`doMaxScore` 控制是否计算整个查询结果中的最高评分。即使 `doTrackScores` 为 `true` 并计算了所有文档的评分，计算最高评分仍然是一个额外的步骤。最高评分可以用于一些特殊场景，比如：在分页的场景中，知道最高评分可以帮助在客户端进行评分的标准化或调整，以便在不同页之间保持一致性。在某些搜索界面中，显示最高评分可以让用户了解查询结果的最高相关性文档，提供更好的用户体验。

**为什么`doTrackScores`计算了评分还要一个参数来控制最高分**

Lucene 在计算每个文档的评分时并不会自动维护一个最高评分的变量。`doMaxScore` 是一个独立的计算步骤，需要在所有评分中找出最高的那个。虽然这听起来可能有些冗余，但它的设计目的是为了在需要最高评分信息时明确地请求它，以便优化性能。

**`doTrackScores`**：在全文检索查询(即相关性查询)中默认开启，在过滤查询和聚合查询中默认关闭。

**`doMaxScore`**：在全文检索查询(即相关性查询)中默认开启，在过滤查询和聚合查询中默认关闭。

相关性查询：`match`，`match_phrase`，`multi_match`，`query_string`，`simple_query_string`

过滤性查询：`term`，`terms`，`range`，`exists`，`prefix`，`wildcard`



#### 索引排序

对于索引排序来说，文档顺序一旦建立就不会再改变，但是如果重新索引文档，先前的文档顺序就不会再有效了，因为新索引的文档会有新的ID，索引最后会根据这些新的ID 进行排序。在没有指定排序方式时，Lucene 就是根据文档的评分来完成排序操作的，而根据文档索引顺序进行的排序操作方式实际上并没有太大作用。



#### 域值排序

在默认情况下，接受 Sort对象参数的search方法不会对匹配文档进行任何评分操作，因为评分操作会消耗大量的系统性能，并且很多程序在通过域排序时并不需要进行评分操作。

通过文本域排序首先要求该域整个被索引成单个语汇单元，一般这需要使用 Field.Index.NOT_ANALYZED 参数或 `Field.Index.NOT_ANALYZED_NO_NORMS` 参数。可以选择是否存储该域，类别域是用参数 `Field.Index.NOT_ANALYZED` 和 `Field.Store.YES` 进行索引的，这样就允许针对该域进行排序。NumericField 实例会为排序而自动进行对应的索引操作。如果需要通过域进行排序，你必须创建一个 Sort 对象，并在对象的初始化方法中指定域名参数，比如：

```java
example.displayResults(query, new Sort(new SortField("category", SortField.STRING)));
```



#### 多字段排序

在创建`Sort`时，其是允许传入多个排序字段的`SortField`枚举。

```java
  /** Sorts in succession by the criteria in each SortField. */
  public Sort(SortField... fields) {
    setSort(fields);
  }
```

实际上，Sort 实例内部保存了一个 SortFields 数组，每个 SortField对象都包含域名、域类型和反向排序标志位。另外，SortField 对象还包含集中域类型的常量，它们是 SCORE、DOC、STRING、 BYTE、 SHORT、 INT、 LONG、 FLOAT和 DOUBLE。其中 SCORE和 DOC 常量是用于针对相关性和文档ID 进行排序的特殊类型。

#### 自定义排序

在`SortField`中，是运行创建自定义的排序的，一下代码来自lucene-core包中的`org.apache.lucene.search.SortField`，`FieldComparatorSource` 是一个抽象类，继承它并实现自定义的排序逻辑。该类的主要目的是提供一种机制，以便 Lucene 在排序文档时，可以使用用户自定义的比较器来替代内置的字段排序。

```java
  /** Creates a sort with a custom comparison function.
   * @param field Name of field to sort by; cannot be <code>null</code>.
   * @param comparator Returns a comparator for sorting hits.
   */
  public SortField(String field, FieldComparatorSource comparator) {
    initFieldType(field, Type.CUSTOM);
    this.comparatorSource = comparator;
  }

  /** Creates a sort, possibly in reverse, with a custom comparison function.
   * @param field Name of field to sort by; cannot be <code>null</code>.
   * @param comparator Returns a comparator for sorting hits.
   * @param reverse True if natural order should be reversed.
   */
  public SortField(String field, FieldComparatorSource comparator, boolean reverse) {
    initFieldType(field, Type.CUSTOM);
    this.reverse = reverse;
    this.comparatorSource = comparator;
  }
```



### MultiPhraseQuery

Lucene 内置的 MultiPhraseQuery 类是一种可以适用于特应用的Query 类，但实际上它还有很多其他用途。MultiPhraseQuery 类与 PhraseQuery类似，区别在于前者允许在同一位置上针对多个项的查询。你也可以通过其他方法完成同样的逻辑功能，但这样会以高昂的系统消耗为代价，如通过使用 BooleanQuery类或者逻辑“或”连接符将所有可能的短语进行联合查询。

比如：

```java
  protected void setUp() throws Exception {
    Directory directory = new RAMDirectory();
    IndexWriter writer = new IndexWriter(directory,
                                         new WhitespaceAnalyzer(),
                                         IndexWriter.MaxFieldLength.UNLIMITED);
    Document doc1 = new Document();
    doc1.add(new Field("field",
              "the quick brown fox jumped over the lazy dog",
              Field.Store.YES, Field.Index.ANALYZED));
    writer.addDocument(doc1);
    Document doc2 = new Document();
    doc2.add(new Field("field",
              "the fast fox hopped over the hound",
              Field.Store.YES, Field.Index.ANALYZED));
    writer.addDocument(doc2);
    writer.close();

    searcher = new IndexSearcher(directory);
  }

  public void testBasic() throws Exception {
    MultiPhraseQuery query = new MultiPhraseQuery();
    query.add(new Term[] {                       // #A
        new Term("field", "quick"),              // #A
        new Term("field", "fast")                // #A
    });
    query.add(new Term("field", "fox"));         // #B
    // field:"(quick fast) fox"，MultiPhraseQuery就是在某个位置支持多个term的匹配
    System.out.println(query);

    TopDocs hits = searcher.search(query, 10);
    assertEquals("fast fox match", 1, hits.totalHits);

    // MultiPhraseQuery也是一样支持slop
    query.setSlop(1);
    hits = searcher.search(query, 10);
    assertEquals("both match", 2, hits.totalHits);
  }
```



### 多字段查询MultiFieldQueryParser

`MultiFieldQueryParser` 是 Lucene 提供的一个实用类，用于在多个字段上解析查询字符串。它的主要功能是支持在多个字段上进行查询，而不需要为每个字段单独编写查询语句。它通常用于需要在多个字段上进行全文搜索的应用场景。

`MultiFieldQueryParser` 将查询字符串解析为 Lucene 的 `Query` 对象，并将其应用到多个字段上。它内部使用 `QueryParser` 对每个字段进行解析，并将生成的查询组合成一个多字段查询。比如，如下的示例：

```java
public void testDefaultOperator() throws Exception {
    Query query = new MultiFieldQueryParser(Version.LUCENE_30,
                                            new String[]{"title", "subject"},
        new SimpleAnalyzer()).parse("development");

    //MultiFieldQueryParser将查询生成为 title:development subject:development
    System.out.println(query);

    Directory dir = TestUtil.getBookIndexDirectory();
    IndexSearcher searcher = new IndexSearcher(dir,true);
    TopDocs hits = searcher.search(query, 10);
    searcher.close();
    dir.close();
  }

  // 设置默认的操作符
  public void testSpecifiedOperator() throws Exception {
    Query query = MultiFieldQueryParser.parse(Version.LUCENE_30,
        "lucene",
        new String[]{"title", "subject"},
        new BooleanClause.Occur[]{BooleanClause.Occur.MUST,
                  BooleanClause.Occur.MUST},
        new SimpleAnalyzer());

    //MultiFieldQueryParser将查询生成为 +title:lucene +subject:lucene
    System.out.println(query);

    Directory dir = TestUtil.getBookIndexDirectory();
    IndexSearcher searcher = new IndexSearcher(dir,true);
    TopDocs hits = searcher.search(query, 10);
    searcher.close();
    dir.close();
  }
```

**`MultiFieldQueryParser` 的配置选项**

1. 字段数组 (`fields`)：指定要搜索的字段。
2. 权重数组 (`boosts`)：指定每个字段的权重，以便在查询时更好地控制相关性。
3. 默认操作符 (`default_operator`)：可以设置为 `AND` 或 `OR`，控制查询字符串中词语之间的默认关系。
4. 允许的查询类型：支持基本查询类型如 `TermQuery`、`PhraseQuery` 和 `BooleanQuery` 等。



### 跨度查询

在搜索期间，跨度查询所跟踪的文档要比匹配的文档要多：每个单独的跨度（每个域可包含多个跨度）都会被跟踪。与 TermQuery 做个比较，TermQuery 只是对文档进行简单的匹配操作，而 SpanTermguery 除了完成这个功能外，还会保留每个匹配文档对应的项位置信息。总的来说，跨度查询是一种计算密集型操作。举例来说，当rermQuery找到包含对应项的文档时，它会记录该匹配文档并且进行下一个文档的查询操作；而 Spanrermuery却必须列举出该项在该文档中所有的出现地点。

SpanQuery 基类包含诸多子类，比如：

| SpanQuery 类型        | 描述                                                         |
| --------------------- | ------------------------------------------------------------ |
| SpanTermQuery         | 和其他跨度查询类型结合使用。单独使用时相当于 TermQuery       |
| SpanFirstQuery        | 用来匹配域中首部分的各个跨度                                 |
| SpanNearQuery         | 用来匹配临近的跨度                                           |
| SpanNotQuery          | 用来匹配不重叠的跨度                                         |
| FieldMaskingSpanQuery | 封装其他 SpanQuery类，但程序会认为已匹配到另外的域。该功能可用于针对多个域的跨度查询 |
| SpanOrQuery           | 跨度查询的聚合匹配                                           |

**SpanTermQuery**：查找在字段field中包含术语`term`的文档。

```java
SpanTermQuery spanTermQuery = new SpanTermQuery(new Term("field", "term"));
```

**SpanFirstQuery**：查找术语出现在文档的前N个位置的文档。

```java
//查找在字段field中术语term出现在前5个位置的文档。
SpanTermQuery spanTermQuery = new SpanTermQuery(new Term("field", "term"));
SpanFirstQuery spanFirstQuery = new SpanFirstQuery(spanTermQuery, 5);
```

**SpanNearQuery**：查找相邻的术语，允许它们之间有一定的距离。

```java
// 查找字段field中包含术语term1和term2的文档，两个术语之间允许最多有5个其他词语，且要求term1必须在term2之前。
SpanTermQuery term1 = new SpanTermQuery(new Term("field", "term1"));
SpanTermQuery term2 = new SpanTermQuery(new Term("field", "term2"));
SpanNearQuery spanNearQuery = new SpanNearQuery(new SpanQuery[]{term1, term2}, 5, true);
```

**SpanNotQuery**：排除包含某些术语的跨度。

```Java
// 查找字段field中包含includeTerm但不包含excludeTerm的文档。pre和post是排除词语前后的宽容度。
SpanTermQuery include = new SpanTermQuery(new Term("field", "includeTerm"));
SpanTermQuery exclude = new SpanTermQuery(new Term("field", "excludeTerm"));
SpanNotQuery spanNotQuery = new SpanNotQuery(include, exclude, 10, 10);
```

**FieldMaskingSpanQuery**：组合不同域的SpanQuery查询，将前面域的SpanQuery查询得到的位置，映射到后续的SpanQuery，以此计算slop。

- **初始查询**: 首先在第一个字段（例如 `title`）上执行 `SpanQuery` 查询，得到匹配词的位置。

- **映射操作**: 使用 `FieldMaskingSpanQuery` 将第一个字段的查询结果的位置映射到第二个字段（例如 `body`）。

- **结合查询**: 在第二个字段上执行另一个 `SpanQuery`，并结合映射的第一个字段的位置，计算 slop 参数，判断是否满足查询条件。

```Java
// 在 title 字段中查找 quick
SpanTermQuery quickInTitle = new SpanTermQuery(new Term("title", "quick"));
// 在 body 字段中查找 fox
SpanTermQuery foxInBody = new SpanTermQuery(new Term("body", "fox"));
// 将 quickInTitle 映射到 body 字段
FieldMaskingSpanQuery maskedQuick = new FieldMaskingSpanQuery(quickInTitle, "body");
// 创建一个 SpanNearQuery，查找 maskedQuick 和 foxInBody 之间最多一个词语的距离
SpanNearQuery spanNearQuery = new SpanNearQuery(new SpanQuery[]{maskedQuick, foxInBody}, 1, true);
```

**SpanOrQuery**：合并多个SpanQuery，类似于布尔查询中的`OR`操作。

```Java
// 查找字段field中包含term1或term2的文档。
SpanTermQuery term1 = new SpanTermQuery(new Term("field", "term1"));
SpanTermQuery term2 = new SpanTermQuery(new Term("field", "term2"));
SpanOrQuery spanOrQuery = new SpanOrQuery(new SpanQuery[]{term1, term2});
```

SpanQuery是可以多个嵌套的，比如

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/lucene/lucene-33.png" alt="image" style={{ maxWidth: '70%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>

[//]: # (<img src="/Users/liufei/docs/typora笔记/images/lucene/lucene-33.png" style="zoom:30%;" />)

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/lucene/lucene-34.png" alt="image" style={{ maxWidth: '80%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>

[//]: # (<img src="/Users/liufei/docs/typora笔记/images/lucene/lucene-34.png" style="zoom:60%;" />)



### 搜索过滤

过滤是 Lucene 中用于缩小搜索空间的一种机制，它把可能的搜索匹配结果仅限制在所有文档的一个子集中。它们可以用来对已经得到的搜索匹配结果进行进一步搜索，以实现在搜索结果中的继续搜索（search-within-search）特性。此外，它们还可以用来限制文档的搜索空间。安全过滤器允许用户只能看见属于“自己的”文档搜索结果，即使这些查询实际上还匹配了其他的文档。

在 Lucene 中，`query` 和 `filter` 的主要区别在于评分操作。传统上，`query` 执行搜索并计算相关性评分，而 `filter` 只用于判断文档是否匹配特定条件，不涉及评分。

传统的 Query 和 Filter 区别

- Query: 用于搜索并计算文档的相关性评分。通常用于全文搜索、布尔查询等。
- Filter: 用于过滤文档，不计算相关性评分。适用于预筛选和缓存结果的场景。

**但在更新的 Lucene 中，`filter` 和 `query` 的区别已经变得模糊。许多过滤器功能已经集成到查询中，使用统一的查询语法来实现过滤和评分。**

#### 更新版本 Lucene 中的评分和过滤

>  **为什么查询还要单独考虑评分并设置过滤**
>
>  虽然`query`和`filter`查询出的结果是相同的，但里面还是有区别，因为标准化的文档评分机制可能也有所不同，因为 IDF （Inverse Document Frequency，倒排文档频率）因子可能是动态改变的。当使用 BooleanQuery集合时，所有包含查询项的文档都被计算人方程中，然而使用过滤器则可以在方程中过滤掉一部分文档，并对倒排文档的频率因子产生一定影响。

但在现代的 Lucene 中，`filter` 和 `query` 的区别已经变得模糊。许多过滤器功能已经集成到查询中，使用统一的查询语法来实现过滤和评分。

- **`ConstantScoreQuery`**: 用于将任意查询包装为一个恒定得分的查询，这样可以将过滤器转换为查询，同时保留评分信息。

```java
Query query = new TermQuery(new Term("field", "value"));
Query filterQuery = new ConstantScoreQuery(query);
  ```

- **`BooleanQuery`**: 允许组合多个子查询和过滤条件，其中可以包含评分和不评分的部分。

```java
BooleanQuery.Builder builder = new BooleanQuery.Builder();
builder.add(new TermQuery(new Term("field", "value")), BooleanClause.Occur.MUST);
builder.add(new ConstantScoreQuery(new TermQuery(new Term("field", "filterValue"))), BooleanClause.Occur.FILTER);
Query query = builder.build();
  ```

`BooleanClause.Occur` 有四种类型：

1. **MUST**: 子查询必须匹配。
2. **MUST_NOT**: 子查询必须不匹配。
3. **SHOULD**: 子查询应该匹配（但不是必须的）。
4. **FILTER**: 子查询必须匹配，但不影响评分（`score`）。

通过`Query`和`BooleanClause.Occur`组合就能替代早期lucene的`Filter`的功能。

> `BooleanClause.Occur` 的一种常量，`BooleanClause`内部就是对`Occur`内部枚举的包装，用于在布尔查询（`BooleanQuery`）中定义子查询的发生器（`Occur`）类型。

比如：

**TermRangeFilter**

TermRangeFilter对特定域的项范围进行过滤，与除去评分功能的termRangeQuery类似，在新版本的 Lucene 中，`TermRangeFilter` 已经被弃用，可以使用 `TermRangeQuery` 来实现相同的功能。

```java
// 后面的两个参数是开闭区间即是否包含边界值，边界值也可以为空
Query query = TermRangeQuery.newStringRange("field", "startTerm", "endTerm", true, true);
```

**NumericRangeFilter**

在较新的 Lucene 版本中，`NumericRangeFilter` 已被 `PointRangeQuery` 取代。

```java
Query query = IntPoint.newRangeQuery("field", 1, 10);
```

**FieldCacheRangeFilter**

`FieldCacheRangeFilter`提供了另一种范围过滤选择。它所完成的过滤功能与`TermRangeFilter` 和 `NumericRangeFilter` 加起来一样，但它的使用却是基于 Lucene的域缓存机制的。使用这个机制可以在某些情况下带来系统性能提升，因为所有的值都提起存储到内存中了，`FieldCacheRangeFilter` 已被弃用，可以使用基于点的范围查询，例如 `PointRangeQuery`。

```java
Query query = IntPoint.newRangeQuery("field", 1, 10);
```

**FieldCacheTermsFilter**

`FieldCacheTermsFilter` 已被弃用，通常可以使用 `TermsQuery` 来实现类似功能。

```java
Query query = new TermsQuery(new Term("field", "value1"), new Term("field", "value2"));
```

**QueryWrapperFilter**

`QueryWrapperFilter` 使用查询中匹配的文档来对随后搜索中可以访问的文档进行限制，它允许你将带有评分功能的查询转换为不带评分功能的过滤器。`QueryWrapperFilter` 已被弃用，可以直接使用 `ConstantScoreQuery` 来包装一个查询，使其成为过滤器。

```java
Query query = new TermQuery(new Term("field", "value"));
Query filterQuery = new ConstantScoreQuery(query);
```

**SpanQueryFilter**

`SpanQueryFilter` 不再使用，可以直接使用 `SpanQuery` 并结合 `BooleanQuery` 来实现复杂的过滤逻辑。

```java
SpanQuery spanQuery = new SpanTermQuery(new Term("field", "value"));
BooleanQuery.Builder builder = new BooleanQuery.Builder();
builder.add(spanQuery, BooleanClause.Occur.FILTER);
Query query = builder.build();
```

**PrefixFilter**

`PrefixFilter` 已被弃用，可以直接使用 `PrefixQuery` 来实现前缀过滤。

```java
Query query = new PrefixQuery(new Term("field", "prefix"));
```

**CachingWrapperFilter**

`CachingWrapperFilter` 已被弃用，新的缓存机制是 `LruQueryCache`。

```java
LruQueryCache queryCache = new LruQueryCache(1000, 100000);
IndexSearcher searcher = new IndexSearcher(reader);
searcher.setQueryCache(queryCache);
Query query = new TermRangeQuery("field", new BytesRef("startTerm"), new BytesRef("endTerm"), true, true);
```

**CachingSpanFilter**

`CachingSpanFilter` 也已被弃用，使用新的 `LruQueryCache` 机制来缓存查询。

```java
LruQueryCache queryCache = new LruQueryCache(1000, 100000);
IndexSearcher searcher = new IndexSearcher(reader);
searcher.setQueryCache(queryCache);
SpanQuery spanQuery = new SpanTermQuery(new Term("field", "value"));
Query cachingQuery = new ConstantScoreQuery(spanQuery);
```

**FilteredDocIdSet**

`FilteredDocIdSet` 已被弃用，新的实现可以使用 `DocIdSetIterator` 来进行自定义过滤。

```java
class CustomDocIdSetIterator extends DocIdSetIterator {
    private final DocIdSetIterator innerIterator;

    CustomDocIdSetIterator(DocIdSetIterator innerIterator) {
        this.innerIterator = innerIterator;
    }

    @Override
    public int docID() {
        return innerIterator.docID();
    }

    @Override
    public int nextDoc() throws IOException {
        int doc;
        while ((doc = innerIterator.nextDoc()) != NO_MORE_DOCS) {
            if (match(doc)) {
                return doc;
            }
        }
        return NO_MORE_DOCS;
    }

    @Override
    public int advance(int target) throws IOException {
        return innerIterator.advance(target);
    }

    @Override
    public long cost() {
        return innerIterator.cost();
    }

    protected boolean match(int docId) {
        // 自定义匹配逻辑
        return docId % 2 == 0; // 例如，只匹配偶数ID的文档
    }
}
```



### 自定义结果评分

在早期的lucene自定义评分中使用的`ValueSourceQuery`现在被`FunctionScoreQuery`和`FeatureField`替代。

- **FunctionScoreQuery**: 用于根据给定的函数对文档进行重新评分。

- **FeatureField**: 用于定义特定特征的值，以便在查询中使用。

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/lucene/lucene-35.png" alt="image" style={{ maxWidth: '50%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>

[//]: # (<img src="/Users/liufei/docs/typora笔记/images/lucene/lucene-35.png" style="zoom:35%;" />)

```java
public class FunctionScoreQueryExample {
    public static void main(String[] args) throws Exception {
        StandardAnalyzer analyzer = new StandardAnalyzer();
        Directory directory = new RAMDirectory();
        IndexWriterConfig config = new IndexWriterConfig(analyzer);
        IndexWriter writer = new IndexWriter(directory, config);

        // 添加文档
        addDoc(writer, "Lucene in Action", "John", 1.0f);
        addDoc(writer, "Lucene for Dummies", "Jane", 2.0f);
        addDoc(writer, "Managing Gigabytes", "John", 3.0f);
        addDoc(writer, "The Art of Computer Science", "Jane", 4.0f);
        writer.close();

        // 创建基本查询
        Query query = new TermQuery(new Term("title", "Lucene"));

        // 使用 FunctionScoreQuery 来重新评分,FloatDocValuesField继承自FieldCacheSource，FieldCacheSource继承自ValueSource
        ValueSource valueSource = new FloatFieldSource("boost");
        FunctionScoreQuery functionScoreQuery = new FunctionScoreQuery(query, valueSource);

        // 执行查询
        DirectoryReader reader = DirectoryReader.open(directory);
        IndexSearcher searcher = new IndexSearcher(reader);
        TopDocs results = searcher.search(functionScoreQuery, 10);

        // 显示结果
        for (ScoreDoc hit : results.scoreDocs) {
            Document doc = searcher.doc(hit.doc);
            System.out.println("Title: " + doc.get("title") + ", Author: " + doc.get("author") + ", Boost: " + doc.get("boost"));
        }

        reader.close();
        directory.close();
    }

    private static void addDoc(IndexWriter writer, String title, String author, float boost) throws Exception {
        Document doc = new Document();
        doc.add(new TextField("title", title, Field.Store.YES));
        doc.add(new TextField("author", author, Field.Store.YES));
        // FloatDocValuesField 是 Lucene 中用于存储 float 类型数值的字段类型。它主要用于为文档添加数值特征，这些数值可以在排序、打分和聚合等操作中使用。
        // FloatDocValuesField和setBoost不同，FloatDocValuesField用于数值计算和操作；setBoost仅影响索引时的权重，主要影响相关性得分
        doc.add(new FloatDocValuesField("boost", boost));
        writer.addDocument(doc);
    }
}

```





### 多索引搜索

常常存在多种情况下，需要去检索多个分离的Lucene 索引，但又需要在搜索过程中能够使针对这几个索引的所有搜索结果合并输出。比如：

- 为了方便程序运行或者管理上的原因——例如，如果不同的用户或者组织不同的文档集合创建了不同的索引，就会导致多个分离索引的出现。
- 为了增大文档容量，对文档进行拆分。例如，一个新闻网站可能每月新创建一个索引，然后在搜索时指定该月份对应的索引即可。

Lucene 早期提供了 `Multisearcher`类，和`ParallelMultiSearcher`类来支持多索引搜索。后续的Lucene `Multisearcher`被`MultiReader`取代，`ParallelMultiSearcher`也被废弃，而是使用`MultiReader` 和 `ExecutorService` 进行并行查询。

而在 Elasticsearch 中，搜索多个索引时默认使用多线程查询，并且会自动合并结果。这是 Elasticsearch 的一个内置特性，旨在提高查询性能和响应速度。

1. **查询拆分**：当对多个索引进行查询时，Elasticsearch 会将查询请求拆分成多个子请求，每个子请求对应一个索引或索引分片。
2. **并行处理**：这些子请求会被分配到不同的线程池和节点，进行并行处理。Elasticsearch 内部有专门的线程池来处理搜索请求，如 `search` 线程池。
3. **结果合并**：各个子请求的结果会在协调节点上进行合并，最终返回给用户一个统一的结果集。



### 项向量

项向量是是一个有关等价存储文档倒排索引的一项高级技术。从技术上讲，**项向量是一组由项-频率对 （term-frequencypair）组成的集合，该向量还可选择性包含各个项出现的位置信息**。项向量可以用于各种高级文本处理任务，如高亮显示、同义词扩展、分类、搜寻相似文档和聚类(自动归类文档)等。比如下图两个文档中包含了cat 和 dog 的项向量：

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/lucene/lucene-36.png" alt="image" style={{ maxWidth: '60%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>

[//]: # (<img src="/Users/liufei/docs/typora笔记/images/lucene/lucene-36.png" style="zoom:50%;" />)

lucene的向量信息都存储在：

- **`Fields` 对象**：表示一个文档中所有字段的集合。

- **`Terms` 对象**：表示一个字段中的所有术语及其统计信息。



如下是功能的示例：

#### 高亮显示

```java
package com.jd.analysis.service.audit.impl;

import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.document.*;
import org.apache.lucene.index.*;
import org.apache.lucene.search.*;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.RAMDirectory;
import org.apache.lucene.search.highlight.*;

/**
 * @author liufei
 * @date 2024/6/26
 **/
public class HighlightExample {
    public static void main(String[] args) throws Exception {
        Directory directory = new RAMDirectory();
        StandardAnalyzer analyzer = new StandardAnalyzer();
        IndexWriterConfig config = new IndexWriterConfig(analyzer);
        IndexWriter writer = new IndexWriter(directory, config);

        // 创建一个字段类型，启用项向量
        FieldType fieldType = new FieldType();
        fieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
        fieldType.setStored(true);
        fieldType.setTokenized(true);
        fieldType.setStoreTermVectors(true);
        fieldType.setStoreTermVectorPositions(true);
        fieldType.setStoreTermVectorOffsets(true);

        // 创建文档并添加字段
        Document doc = new Document();
        doc.add(new Field("content", "Lucene is a powerful search library written in Java.", fieldType));
        writer.addDocument(doc);
        writer.close();

        // 搜索
        DirectoryReader reader = DirectoryReader.open(directory);
        IndexSearcher searcher = new IndexSearcher(reader);
        Query query = new TermQuery(new Term("content", "lucene"));
        TopDocs topDocs = searcher.search(query, 10);

        // 高亮显示
        SimpleHTMLFormatter htmlFormatter = new SimpleHTMLFormatter();
        Highlighter highlighter = new Highlighter(htmlFormatter, new QueryScorer(query));
        for (ScoreDoc scoreDoc : topDocs.scoreDocs) {
            Document hitDoc = searcher.doc(scoreDoc.doc);
            String text = hitDoc.get("content");
            TokenStream tokenStream = TokenSources.getTermVectorTokenStreamOrNull("content", reader.getTermVectors(scoreDoc.doc), -1);
            TextFragment[] frag = highlighter.getBestTextFragments(tokenStream, text, false, 10);
            for (TextFragment textFrag : frag) {
                System.out.println(textFrag.toString());
            }
        }
        reader.close();
        directory.close();
    }
}

// 输出内容
// <B>Lucene</B> is a powerful search library written in Java.
```

#### 相似度计算

计算相似度，需要计算向量之间的夹角，向量夹角公式为：

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/lucene/lucene-37.png" alt="image" style={{ maxWidth: '70%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>

[//]: # (<img src="/Users/liufei/docs/typora笔记/images/lucene/lucene-37.png" style="zoom:50%;" />)

其就是线性代数中的向量夹角公式：·

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/lucene/cosTheta.jpg" alt="image" style={{ maxWidth: '80%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>


```java
package com.jd.analysis.service.test;

/**
 * @author liufei
 * @date 2024/6/26
 **/
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.document.*;
import org.apache.lucene.index.*;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.RAMDirectory;
import org.apache.lucene.util.BytesRef;
import org.apache.lucene.util.PriorityQueue;

public class SimilarityExample {
    public static void main(String[] args) throws Exception {
        Directory directory = new RAMDirectory();
        StandardAnalyzer analyzer = new StandardAnalyzer();
        IndexWriterConfig config = new IndexWriterConfig(analyzer);
        IndexWriter writer = new IndexWriter(directory, config);

        // 创建一个字段类型，启用项向量
        FieldType fieldType = new FieldType();
        fieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS);
        fieldType.setStored(true);
        fieldType.setTokenized(true);
        fieldType.setStoreTermVectors(true);

        // 创建文档并添加字段
        Document doc1 = new Document();
        doc1.add(new Field("content", "Lucene is a powerful search library written in Java.", fieldType));
        writer.addDocument(doc1);

        Document doc2 = new Document();
        doc2.add(new Field("content", "Elasticsearch is built on top of Lucene.", fieldType));
        writer.addDocument(doc2);

        writer.close();

        // 计算相似度
        DirectoryReader reader = DirectoryReader.open(directory);
        Terms terms1 = reader.getTermVector(0, "content");
        Terms terms2 = reader.getTermVector(1, "content");

        double similarity = computeCosineSimilarity(terms1, terms2);
        System.out.println("Similarity: " + similarity);

        reader.close();
        directory.close();
    }

    // 计算向量的夹角，来获取相似度，1就是完全相同，0就是两个向量垂直完全不相同
    private static double computeCosineSimilarity(Terms terms1, Terms terms2) throws Exception {
        TermsEnum termsEnum1 = terms1.iterator();
        TermsEnum termsEnum2 = terms2.iterator();

        BytesRef term1;
        BytesRef term2;

        double dotProduct = 0.0;
        double norm1 = 0.0;
        double norm2 = 0.0;

        while ((term1 = termsEnum1.next()) != null) {
            if (termsEnum2.seekExact(term1)) {
                long freq1 = termsEnum1.totalTermFreq();
                long freq2 = termsEnum2.totalTermFreq();
                dotProduct += freq1 * freq2;
                norm1 += Math.pow(freq1, 2);
                norm2 += Math.pow(freq2, 2);
            }
        }

        return dotProduct / (Math.sqrt(norm1) * Math.sqrt(norm2));
    }
}

// 输出：Similarity: 1.0
```

#### 分类和聚类

```java
package com.jd.analysis.service.test;

/**
 * @author liufei
 * @date 2024/6/26
 **/
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.document.*;
import org.apache.lucene.index.*;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.RAMDirectory;
import org.apache.lucene.util.BytesRef;
import org.apache.lucene.util.PriorityQueue;

import java.util.HashMap;
import java.util.Map;

public class ClassificationExample {
    public static void main(String[] args) throws Exception {
        Directory directory = new RAMDirectory();
        StandardAnalyzer analyzer = new StandardAnalyzer();
        IndexWriterConfig config = new IndexWriterConfig(analyzer);
        IndexWriter writer = new IndexWriter(directory, config);

        // 创建一个字段类型，启用项向量
        FieldType fieldType = new FieldType();
        fieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS);
        fieldType.setStored(true);
        fieldType.setTokenized(true);
        fieldType.setStoreTermVectors(true);

        // 创建文档并添加字段
        Document doc1 = new Document();
        doc1.add(new Field("content", "Lucene is a powerful search library written in Java.", fieldType));
        writer.addDocument(doc1);

        Document doc2 = new Document();
        doc2.add(new Field("content", "Elasticsearch is built on top of Lucene.", fieldType));
        writer.addDocument(doc2);

        Document doc3 = new Document();
        doc3.add(new Field("content", "Lucene and Elasticsearch are both open source projects.", fieldType));
        writer.addDocument(doc3);

        writer.close();

        // 读取索引并获取项向量
        DirectoryReader reader = DirectoryReader.open(directory);
        Map<String, Integer> termFreqMap1 = getTermFrequency(reader, 0);
        Map<String, Integer> termFreqMap2 = getTermFrequency(reader, 1);
        Map<String, Integer> termFreqMap3 = getTermFrequency(reader, 2);

        // 简单分类示例：根据术语频率进行分类
        classifyDocument(termFreqMap1);
        classifyDocument(termFreqMap2);
        classifyDocument(termFreqMap3);

        reader.close();
        directory.close();
    }

    private static Map<String, Integer> getTermFrequency(DirectoryReader reader, int docId) throws Exception {
        Terms terms = reader.getTermVector(docId, "content");
        TermsEnum termsEnum = terms.iterator();
        BytesRef term;

        Map<String, Integer> termFreqMap = new HashMap<>();
        while ((term = termsEnum.next()) != null) {
            termFreqMap.put(term.utf8ToString(), (int) termsEnum.totalTermFreq());
        }
        return termFreqMap;
    }

    private static void classifyDocument(Map<String, Integer> termFreqMap) {
        if (termFreqMap.containsKey("elasticsearch")) {
            System.out.println("Document is related to Elasticsearch.");
        } else if (termFreqMap.containsKey("lucene")) {
            System.out.println("Document is related to Lucene.");
        } else {
            System.out.println("Document is of unknown category.");
        }
    }
}

// 输出：
// Document is related to Lucene.
// Document is related to Elasticsearch.
// Document is related to Elasticsearch.
```



#### 相似文档查询

查询文档中指定内容的相似文档，可以通过`Terms`获取到对应字段的所有term，然后将term作为查询条件去`should`关联查询，最后得到相似的文档。因为本身es查询时就会计算相似度（[相关性排序](####相关性排序)），所以此处多个term的查询条件查询出来的，就是有更多term相似的文档。

```java
package com.jd.analysis.service.test;

import org.apache.lucene.document.Document;
import org.apache.lucene.index.*;
import org.apache.lucene.search.*;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.FSDirectory;

import java.io.IOException;
import java.nio.file.Paths;

/**
 * 查询步骤：
 * 获取指定文档的内容：从索引中获取指定文档。
 * 提取指定字段的内容：获取指定字段的内容，这些内容将用作查询条件。
 * 构建查询：基于提取的内容构建查询条件。
 * 执行查询：使用构建的查询条件在索引中查找相似的文档。
 *
 * 关键点：
 * 将获取的文档，获取对应field的terms，然后构建should查询
 * 因为lucene相关性查询本身会计算相关性得分，所以能查询出相似的文档
 *
 * @author liufei
 * @date 2024/6/27
 **/
public class BooksLikeThis {

    public static void main(String[] args) throws IOException {
        Directory dir = FSDirectory.open(Paths.get("path_to_index_directory"));
        try (IndexReader reader = DirectoryReader.open(dir)) {
            int numDocs = reader.maxDoc();

            BooksLikeThis blt = new BooksLikeThis(reader);
            for (int i = 0; i < numDocs; i++) {
                System.out.println();
                Document doc = reader.document(i);
                System.out.println("Original document title: " + doc.get("title"));

                Document[] docs = blt.docsLike(i, 10);
                if (docs.length == 0) {
                    System.out.println("  None like this");
                }
                for (Document likeThisDoc : docs) {
                    System.out.println("  -> " + likeThisDoc.get("title"));
                }
            }
        }
    }

    private final IndexReader reader;
    private final IndexSearcher searcher;

    public BooksLikeThis(IndexReader reader) {
        this.reader = reader;
        this.searcher = new IndexSearcher(reader);
    }

    public Document[] docsLike(int docId, int max) throws IOException {
        Document doc = reader.document(docId);

        String[] authors = doc.getValues("author");
        BooleanQuery.Builder authorQuery = new BooleanQuery.Builder();
        for (String author : authors) {
            authorQuery.add(new TermQuery(new Term("author", author)), BooleanClause.Occur.SHOULD);
        }
        // 提升作者的查询权重
        BoostQuery authorBoost = new BoostQuery(authorQuery.build(), 2.0f);

        // 构建查询条件
        Terms terms = reader.getTermVector(docId, "subject");
        BooleanQuery.Builder subjectQuery = new BooleanQuery.Builder();
        if (terms != null) {
            TermsEnum termsEnum = terms.iterator();
            while (termsEnum.next() != null) {
                TermQuery tq = new TermQuery(new Term("subject", termsEnum.term().utf8ToString()));
                subjectQuery.add(tq, BooleanClause.Occur.SHOULD);
            }
        }

        // 拼装查询条件
        BooleanQuery.Builder likeThisQuery = new BooleanQuery.Builder();
        likeThisQuery.add(authorBoost, BooleanClause.Occur.SHOULD);
        likeThisQuery.add(subjectQuery.build(), BooleanClause.Occur.SHOULD);
        likeThisQuery.add(new TermQuery(new Term("isbn", doc.get("isbn"))), BooleanClause.Occur.MUST_NOT);

        TopDocs hits = searcher.search(likeThisQuery.build(), max);
        int size = Math.min(max, hits.scoreDocs.length);

        Document[] docs = new Document[size];
        for (int i = 0; i < size; i++) {
            docs[i] = reader.document(hits.scoreDocs[i].doc);
        }

        return docs;
    }
}
```

但以上相关性查询，有个缺陷就是查询条件确实会丢失每个 term 的词频和位置信息。简单地说，这种方法只关注于那些出现在指定文档域中的 terms，而忽略了这些 terms 的频率（term frequency）和它们在文档中的位置（position），而这两个因素在计算相关性时是非常重要的。

在 Lucene 中，相关性评分（例如 TF-IDF，BM25）是通过考虑 term frequency（词频），document frequency（文档频率）以及 terms 的位置来计算的。通过只使用 terms 构建查询，丢失了这些信息，从而影响了相关性的计算。可以使用`MoreLikeThis`来改进此问题。

`MoreLikeThis`是`org.apache.lucene:lucene-queries`提供的工具类，下面是

```java
public class MoreLikeThisExample {
    public static void main(String[] args) throws IOException {
        IndexReader reader = ...; // 初始化 IndexReader
        IndexSearcher searcher = new IndexSearcher(reader);

        MoreLikeThis moreLikeThis = new MoreLikeThis(reader);
        moreLikeThis.setFieldNames(new String[] {"subject"});
        moreLikeThis.setMinTermFreq(1);
        moreLikeThis.setMinDocFreq(1);

        // 获取指定文档的内容
        int docId = ...; // 需要查找相似文档的文档 ID
        Terms terms = reader.getTermVector(docId, "subject");

        // 构建查询
        Query likeQuery = moreLikeThis.like(docId);

        // 执行查询
        TopDocs topDocs = searcher.search(likeQuery, 10);
        for (int i = 0; i < topDocs.scoreDocs.length; i++) {
            System.out.println("DocID: " + topDocs.scoreDocs[i].doc);
        }
    }
}
```



### 访问指定字段

在早期lucene中可以使用`FieldSelector`来访问部分字段，在更新版的 Lucene 中，`FieldSelector` 类已经被移除，替代它的功能现在由 `StoredFieldVisitor` 来实现。`StoredFieldVisitor` 提供了更灵活和高效的方式来选择和访问存储的字段。

**提高查询效率**：通过只访问必要的字段，可以减少从磁盘读取的数据量，从而加快查询速度。

**减少内存损耗**：只加载必要的字段数据到内存中，减少内存的使用量，特别是当文档中包含大量不需要的字段时。

```java
public class SpecificFieldVisitorExample {
    public static void main(String[] args) throws IOException {
        Directory dir = FSDirectory.open(Paths.get("/path/to/index"));
        IndexReader reader = DirectoryReader.open(dir);

        int docId = 0; // Document ID for which to fetch stored fields

        // 定义要访问的字段
        StoredFieldVisitor visitor = new StoredFieldVisitor() {
            @Override
            public void stringField(FieldInfo fieldInfo, String value) throws IOException {
                System.out.println("Field: " + fieldInfo.name + " - Value: " + value);
            }

            @Override
            public Status needsField(FieldInfo fieldInfo) throws IOException {
                // Specify which fields to load
                if ("fieldName1".equals(fieldInfo.name) || "fieldName2".equals(fieldInfo.name)) {
                    return Status.YES;
                }
                return Status.NO;
            }
        };

        // Visit the stored fields of the document
        reader.document(docId, visitor);

        reader.close();
        dir.close();
    }
}
```

> 注意：域加载期间大量的开销会用在找寻索引中域存储位置所对应的指针上，因此你可能会发现就算跳过对一些域的加载也不会节省大量的时间。具体实施时需要对自己的应用程序进行测试，以找到最佳的性能取舍。




## lucene搜索自定义扩展

### 自定排序

一般的排序，在lucene的查询中就可以指定排序字段：

```java
// 创建自定义排序（按价格排序）
Sort sort = new Sort(new SortField("price", SortField.Type.FLOAT));
// 执行搜索
TopDocs results = searcher.search(query, 10, sort);
```

而自定义排序步骤需要如下：

1. 继承 `FieldComparator` 类：创建一个继承 `FieldComparator` 类的自定义比较器。
2. 实现 `FieldComparatorSource` 类：创建一个继承 `FieldComparatorSource` 类的自定义比较器源。
3. 在查询中使用自定义排序：将自定义排序器应用到查询中。

例子和解释如下：

构建索引

```java
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.document.Document;
import org.apache.lucene.document.Field;
import org.apache.lucene.document.FloatPoint;
import org.apache.lucene.document.StoredField;
import org.apache.lucene.document.StringField;
import org.apache.lucene.index.DirectoryReader;
import org.apache.lucene.index.IndexWriter;
import org.apache.lucene.index.IndexWriterConfig;
import org.apache.lucene.search.*;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.RAMDirectory;

import java.io.IOException;

public class LuceneCustomSortExample {
    public static void main(String[] args) throws IOException {
        // 创建内存索引
        Directory directory = new RAMDirectory();
        StandardAnalyzer analyzer = new StandardAnalyzer();
        IndexWriterConfig config = new IndexWriterConfig(analyzer);
        IndexWriter writer = new IndexWriter(directory, config);

        // 添加商品文档
        addDoc(writer, "Product A", 10.99f);
        addDoc(writer, "Product B", 8.99f);
        addDoc(writer, "Product C", 12.99f);
        writer.close();

        // 读取索引并进行自定义排序查询
        DirectoryReader reader = DirectoryReader.open(directory);
        IndexSearcher searcher = new IndexSearcher(reader);

        // 构建查询（这里我们使用MatchAllDocsQuery以匹配所有文档）
        Query query = new MatchAllDocsQuery();

        // 创建自定义排序（按价格排序）
        Sort sort = new Sort(new SortField("price", new PriceComparatorSource()));

        // 执行搜索
        TopDocs results = searcher.search(query, 10, sort);
        for (ScoreDoc scoreDoc : results.scoreDocs) {
            Document doc = searcher.doc(scoreDoc.doc);
            System.out.println(doc.get("name") + ": " + doc.get("price"));
        }
        reader.close();
        directory.close();
    }

    private static void addDoc(IndexWriter writer, String name, float price) throws IOException {
        Document doc = new Document();
        doc.add(new StringField("name", name, Field.Store.YES));
        doc.add(new FloatPoint("price", price));
        doc.add(new StoredField("price", price));
        writer.addDocument(doc);
    }
}
```



自定义 Comparator Source

```java
import org.apache.lucene.index.LeafReaderContext;
import org.apache.lucene.search.FieldComparator;
import org.apache.lucene.search.FieldComparatorSource;
import org.apache.lucene.search.LeafFieldComparator;
import org.apache.lucene.util.BytesRef;

import java.io.IOException;

class PriceComparatorSource extends FieldComparatorSource {
    @Override
    public FieldComparator<Float> newComparator(String fieldname, int numHits, int sortPos, boolean reversed) {
        return new PriceComparator(numHits, fieldname);
    }
}

class PriceComparator extends FieldComparator<Float> {
    private final String field;
    private final float[] values;
    private float bottom;
    private float topValue;
    private BinaryDocValues currentValues;

    public PriceComparator(int numHits, String field) {
        this.field = field;
        this.values = new float[numHits];
    }

    @Override
    public LeafFieldComparator getLeafComparator(LeafReaderContext context) throws IOException {
        currentValues = context.reader().getBinaryDocValues(field);
        return new PriceLeafFieldComparator();
    }

    @Override
    public int compare(int slot1, int slot2) {
        return Float.compare(values[slot1], values[slot2]);
    }

    @Override
    public void setTopValue(Float value) {
        this.topValue = value;
    }

    @Override
    public Float value(int slot) {
        return values[slot];
    }

    @Override
    public void setBottom(int slot) {
        bottom = values[slot];
    }

    private class PriceLeafFieldComparator implements LeafFieldComparator {
        @Override
        public int compareBottom(int doc) throws IOException {
            float docValue = FloatPoint.decodeDimension(currentValues.binaryValue().bytes, 0);
            return Float.compare(bottom, docValue);
        }

        @Override
        public void copy(int slot, int doc) throws IOException {
            values[slot] = FloatPoint.decodeDimension(currentValues.binaryValue().bytes, 0);
        }

        @Override
        public void setScorer(Scorable scorer) {
            // Not needed for this example
        }

        @Override
        public void setTopValue(Float value) {
            topValue = value;
        }

        @Override
        public int compareTop(int doc) throws IOException {
            float docValue = FloatPoint.decodeDimension(currentValues.binaryValue().bytes, 0);
            return Float.compare(topValue, docValue);
        }
    }
}
```

> **代码解释**：`PriceComparatorSource` 和 `PriceComparator` 用于自定义排序。`PriceComparatorSource` 创建一个新的 `PriceComparator` 实例，`PriceComparator` 实现了比较逻辑。
>
> **`PriceComparator`**：
>
> - **`getLeafComparator`**：返回一个新的 `LeafFieldComparator` 实例，并获取当前文档的值（`currentValues`）。
> - **`compare`**：比较两个槽位的值。
> - **`setTopValue`**：设置当前最大的比较值。
> - **`value`**：返回指定槽位的值。
> - **`setBottom`**：设置当前最小的比较值。
>
> **`PriceLeafFieldComparator`**：
>
> - **`compareBottom`**：与当前文档的值进行比较。
> - **`copy`**：将文档值复制到指定槽位。
> - **`setScorer`**：设置评分器，在这个例子中不需要实现。
> - **`setTopValue`**：设置当前最大的比较值。
> - **`compareTop`**：与当前文档的值进行比较。
>
> **查询比较的逻辑**
>
> `values[slot]` 数组存储的是排序过程中保存的结果槽位数据。具体来说，当 Lucene 进行自定义排序时，会将每个文档的值存储在 `values` 数组的槽位中，然后通过 `setTopValue` 和 `setBottom` 方法来设置当前比较的基准值（顶值和底值），并通过 `compare` 方法进行比较和排序。最终，`values` 数组中会保存排序后的文档值。



**获取排序后的结果额外信息**

在上面的查询的自定义排序返回的`TopDocs`，肯还需要额外的信息，此时就可以使用`TopFieldDocs`，使用 `TopFieldDocs` 可以在搜索结果中获取排序字段的具体值，这在需要进一步处理排序信息时非常有用。

`TopDocs` 是一个简单的搜索结果容器，包含了搜索命中的文档信息，但不包含任何关于排序字段的信息。它主要有以下几个字段：

- `totalHits`：总命中文档数。
- `scoreDocs`：包含命中文档的数组，每个 `ScoreDoc` 包含文档 ID 和评分。

`TopFieldDocs` 是 `TopDocs` 的一个子类，它除了包含 `TopDocs` 的所有字段外，还包含排序字段的信息。它主要有以下几个字段：

- `totalHits`：总命中文档数。
- `scoreDocs`：包含命中文档的数组，每个 `FieldDoc`（`ScoreDoc` 的子类）包含文档 ID、评分和排序字段值。
- `fields`：包含排序字段的信息（即 `SortField[]`）。



### 结果收集器

Collector 是一个抽象基类，它为 Lucene 定义了用于搜索期间与之进行交互的API。Lucene 所有的核心搜索方法都在后台使用Collector 子类来完成搜索结果收集。举例来说，当通过相关性进行排序时，后台会使用TopScoreDocCollector 类。当通过域进行排序时，则是使用 TopFieldCollector。以上两个公开类都存在于 org.apache.lucene.search 程序包中，你可以在需要的时候再对它们进行实例化。

### Collector

`Collector` 是一个抽象类，用于定义如何收集搜索结果。它在整个搜索过程（跨多个索引段）中起作用。主要方法有：

- `boolean needsScores()`：此收集器是否需要评分。
- `getLeafCollector(LeafReaderContext context)`：返回一个 `LeafCollector`，用于处理特定索引段（`LeafReaderContext`）。

### LeafCollector

`LeafCollector` 是一个接口，定义了如何在单个索引段中收集搜索结果。主要方法有：

- `setScorer(Scorer scorer)`：设置评分器，用于获取文档评分。
- `collect(int doc)`：收集命中的文档。`doc` 是当前索引段中的文档ID。

```java
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.document.*;
import org.apache.lucene.index.DirectoryReader;
import org.apache.lucene.index.IndexWriter;
import org.apache.lucene.index.IndexWriterConfig;
import org.apache.lucene.search.*;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.RAMDirectory;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

public class CustomCollectorExample {

    public static void main(String[] args) throws IOException {
        // 创建内存索引
        Directory directory = new RAMDirectory();
        StandardAnalyzer analyzer = new StandardAnalyzer();
        IndexWriterConfig config = new IndexWriterConfig(analyzer);
        IndexWriter writer = new IndexWriter(directory, config);

        // 添加文档
        addDoc(writer, "Product A", 10.99f);
        addDoc(writer, "Product B", 8.99f);
        addDoc(writer, "Product C", 12.99f);
        writer.close();

        // 读取索引并进行查询
        DirectoryReader reader = DirectoryReader.open(directory);
        IndexSearcher searcher = new IndexSearcher(reader);

        // 构建查询（这里我们使用MatchAllDocsQuery以匹配所有文档）
        Query query = new MatchAllDocsQuery();

        // 创建自定义Collector
        CustomCollector collector = new CustomCollector();

        // 执行搜索
        searcher.search(query, collector);

        // 打印收集到的结果
        System.out.println("Total hits: " + collector.getTotalHits());
        System.out.println("Total score: " + collector.getTotalScore());
        System.out.println("Doc IDs: " + collector.getDocIds());

        reader.close();
        directory.close();
    }

    private static void addDoc(IndexWriter writer, String name, float price) throws IOException {
        Document doc = new Document();
        doc.add(new StringField("name", name, Field.Store.YES));
        doc.add(new FloatPoint("price", price));
        doc.add(new StoredField("price", price));
        writer.addDocument(doc);
    }

    // 自定义Collector
    static class CustomCollector extends SimpleCollector {
        private Scorable scorer;
        private int totalHits;
        private float totalScore;
        private final List<Integer> docIds = new ArrayList<>();

        @Override
        public boolean needsScores() {
            return true;
        }

        @Override
        public void setScorer(Scorable scorer) {
            this.scorer = scorer;
        }

        @Override
        public void collect(int doc) throws IOException {
            totalHits++;
            totalScore += scorer.score();
            docIds.add(doc);
        }

        public int getTotalHits() {
            return totalHits;
        }

        public float getTotalScore() {
            return totalScore;
        }

        public List<Integer> getDocIds() {
            return docIds;
        }
    }
}
```



### 自定义查询过滤器

早期lucene包中`org.apache.lucene.search.Filter`接口提供了查询过滤的功能，但是，从 Lucene 5.5 开始，`Filter` 被弃用，并最终在 Lucene 6.0 中被移除。`Filter` 的功能被合并到了 `Query`中，因此在现代 Lucene 中，我们使用 `Query` 来实现过滤功能。

具体来说，`Filter` 的功能被 `Query` 的 `ConstantScoreQuery` 和 `BooleanQuery` 的 `must` 子句取代。`ConstantScoreQuery` 包装一个查询并使其所有匹配文档都具有相同的评分，从而实现过滤器的效果。而且`BooleanClause.Occur.FILTER`也提供了过滤的功能，如下：

```java
public class CustomFilterExample {
    public static void main(String[] args) throws IOException {
        // 创建内存索引
        Directory directory = new RAMDirectory();
        StandardAnalyzer analyzer = new StandardAnalyzer();
        IndexWriterConfig config = new IndexWriterConfig(analyzer);
        IndexWriter writer = new IndexWriter(directory, config);

        // 添加文档
        addDoc(writer, "20230101", "Document 1");
        addDoc(writer, "20230201", "Document 2");
        addDoc(writer, "20230301", "Document 3");
        writer.close();

        // 读取索引并进行查询
        DirectoryReader reader = DirectoryReader.open(directory);
        IndexSearcher searcher = new IndexSearcher(reader);

        // 构建查询（这里我们使用MatchAllDocsQuery以匹配所有文档）
        Query query = new MatchAllDocsQuery();

        // 创建日期范围查询（用于过滤）
        Query dateRangeQuery = TermRangeQuery.newStringRange("date", "20230201", "20230301", true, true);

        // 构建布尔查询，将主查询和过滤查询结合在一起
        BooleanQuery.Builder booleanQuery = new BooleanQuery.Builder();
        booleanQuery.add(query, BooleanClause.Occur.MUST);
        booleanQuery.add(dateRangeQuery, BooleanClause.Occur.FILTER);

        // 执行搜索
        TopDocs results = searcher.search(booleanQuery.build(), 10);
        for (ScoreDoc scoreDoc : results.scoreDocs) {
            Document doc = searcher.doc(scoreDoc.doc);
            System.out.println("Doc ID: " + scoreDoc.doc + ", Date: " + doc.get("date") + ", Content: " + doc.get("content"));
        }

        reader.close();
        directory.close();
    }

    private static void addDoc(IndexWriter writer, String date, String content) throws IOException {
        Document doc = new Document();
        doc.add(new StringField("date", date, Field.Store.YES));
        doc.add(new TextField("content", content, Field.Store.YES));
        writer.addDocument(doc);
    }
}
```



### Payloads

有效载荷是Lucene 一个高级功能，它使应用程序能够针对索引期间出现的项保存任意数量的字节数组。该字节数组对于 Lucene 是完全不透明的：它只是在索引期间简单地存储每个项的位置信息，然后将这些信息用于随后的搜索。另外，Lucene 核心功能模块并不使用有效载荷进行任何操作，也不对这些内容作出任何假设。这意味着你可以存储任意数量的对程序较为重要的编码数据，并在随后的搜索中使用这些数据，或者在程序中判断哪些文档存在于搜索结果中，或者判断这些匹配文档是如何进行评分和排序的。

有效载荷有多种用途。比如：

- 基于同一个项在文档中的出现位置而对它进行不用的加权
- 对索引中每个项的词性信息进行存储并基于这些信息来改变项的过滤、评分或排序方式。

具体的用法比如：一些文档是公告内容（weather warning）而其他文档则为更为普通的形式，那么你可以通过搜索“warning”来对公告文档中出现的项进行特殊加权。或者就是对原始文本中出现的粗体或斜体项进行加权，或者对HTML 文档中包含title 标签或 header 标签的项进行加权。*尽管你可以使用域加权操作完成这些功能，但这种方式要求你将所有重要的项拆分成各个分离的域，而通常这是不可行或没必要的。有效载特性使你能够基于单个域中各个项的信息进行加权操作。*



#### 分析中生成Payloads

创建自定义 `TokenFilter` 以添加 Payload

```java
import org.apache.lucene.analysis.TokenFilter;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
import org.apache.lucene.util.BytesRef;

import java.io.IOException;

public class PayloadTokenFilter extends TokenFilter {
    private final CharTermAttribute charTermAttr = addAttribute(CharTermAttribute.class);
    private final PayloadAttribute payloadAttr = addAttribute(PayloadAttribute.class);

    protected PayloadTokenFilter(TokenStream input) {
        super(input);
    }

    @Override
    public final boolean incrementToken() throws IOException {
        if (input.incrementToken()) {
            String term = charTermAttr.toString();
            // 这里我们将术语长度作为 Payload 的内容
            payloadAttr.setPayload(new BytesRef(term.getBytes().length));
            return true;
        }
        return false;
    }
}
```

> 核心就是重新incrementToken方法，以便在生成每个令牌token时执行特定的逻辑
>
> `incrementToken()` 方法的主要作用是：
>
> - **生成下一个令牌**：每次调用 `incrementToken()` 方法时，它会将流定位到下一个令牌，并更新所有相关的属性（如字符、位置、偏移量等）。
> - **检查流的结束**：如果流中没有更多的令牌可供生成，则 `incrementToken()` 方法返回 `false`，表示流已经结束。



自定义一个分析器，使用代理模式来使用已有的分析器，将`payloadTokenFilter`添加到分析的过程中

```java
import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.analysis.TokenFilter;
import org.apache.lucene.analysis.TokenStream;

public class PayloadAnalyzer extends Analyzer {
    private final Analyzer delegateAnalyzer;

    public PayloadAnalyzer(Analyzer delegateAnalyzer) {
        this.delegateAnalyzer = delegateAnalyzer;
    }

    @Override
    protected TokenStreamComponents createComponents(String fieldName) {
        TokenStreamComponents components = delegateAnalyzer.createComponents(fieldName);
        TokenStream tokenStream = components.getTokenStream();
        TokenFilter payloadTokenFilter = new PayloadTokenFilter(tokenStream);
        return new TokenStreamComponents(components.getTokenizer(), payloadTokenFilter);
    }
}
```

创建索引并添加带有 Payload 的文档

```java
import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.document.Document;
import org.apache.lucene.document.Field;
import org.apache.lucene.document.TextField;
import org.apache.lucene.index.DirectoryReader;
import org.apache.lucene.index.IndexWriter;
import org.apache.lucene.index.IndexWriterConfig;
import org.apache.lucene.search.IndexSearcher;
import org.apache.lucene.search.MatchAllDocsQuery;
import org.apache.lucene.search.TopDocs;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.RAMDirectory;

import java.io.IOException;

public class PayloadExample {
    public static void main(String[] args) throws IOException {
        Directory directory = new RAMDirectory();
        Analyzer standardAnalyzer = new StandardAnalyzer();
        Analyzer payloadAnalyzer = new PayloadAnalyzer(standardAnalyzer);

        IndexWriterConfig config = new IndexWriterConfig(payloadAnalyzer);
        IndexWriter writer = new IndexWriter(directory, config);

        // 添加文档
        Document doc = new Document();
        doc.add(new TextField("content", "Lucene is a powerful search library", Field.Store.YES));
        writer.addDocument(doc);
        writer.close();

        // 读取索引并进行查询
        DirectoryReader reader = DirectoryReader.open(directory);
        IndexSearcher searcher = new IndexSearcher(reader);

        // 构建查询（匹配所有文档）
        MatchAllDocsQuery query = new MatchAllDocsQuery();

        // 执行搜索
        TopDocs results = searcher.search(query, 10);
        for (var scoreDoc : results.scoreDocs) {
            Document resultDoc = searcher.doc(scoreDoc.doc);
            System.out.println("Doc ID: " + scoreDoc.doc + ", Content: " + resultDoc.get("content"));
        }

        reader.close();
        directory.close();
    }
}
```



#### 搜索时使用Payloads

在查询过程中，我们可以通过自定义 `Similarity` 或 `Collector` 来读取和使用 `Payload`。以下示例展示了如何自定义 `Similarity` 以使用 `Payload` 来调整评分。

```java
import org.apache.lucene.index.FieldInvertState;
import org.apache.lucene.index.LeafReaderContext;
import org.apache.lucene.search.CollectionStatistics;
import org.apache.lucene.search.TermStatistics;
import org.apache.lucene.search.similarities.ClassicSimilarity;
import org.apache.lucene.search.similarities.Similarity;

import java.io.IOException;

public class PayloadSimilarity extends ClassicSimilarity {

    @Override
    public float scorePayload(int docID, int start, int end, BytesRef payload) {
        if (payload != null) {
            return payload.bytes[payload.offset]; // 使用Payload的值调整评分
        }
        return 1.0f;
    }

    @Override
    public void computeNorm(FieldInvertState state, Norm norm) {
        super.computeNorm(state, norm);
    }

    @Override
    public SimScorer simScorer(SimWeight weight, LeafReaderContext context) throws IOException {
        return new SimScorer() {
            @Override
            public float score(int doc, float freq) throws IOException {
                return freq; // 基于词频的评分
            }

            @Override
            public float computeSlopFactor(int distance) {
                return 1.0f; // 斜率因子
            }

            @Override
            public float computePayloadFactor(int doc, int start, int end, BytesRef payload) {
                return scorePayload(doc, start, end, payload); // 使用Payload因子
            }
        };
    }
}
```

并且在搜索中设置自定义的`PayloadSimilarity`

```java
// 设置自定义相似性
searcher.setSimilarity(new PayloadSimilarity());
// 执行搜索
TopDocs results = searcher.search(query, 10);
```











