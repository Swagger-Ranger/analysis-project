---
title: data-structure-哈希
sidebar_position: 11
toc_min_heading_level: 2
toc_max_heading_level: 5
---
## 静态哈希表

散列表也可以用作索引。有的散列表包含许多的记录，以至于它们主要存放在辅助存储器上。对于存放在辅助存储器上的散列表，每一个桶由存储块组成。

通过散列函数h得到的散列值，这个散列值定位到某个桶。如果桶中有太多的记录，**那么可以给该桶加溢出块的链以存放更多的记录** 。

前面说的散列函数，该函数的计算规则如下：

- 1、当键为整数时，散列函数的一种常见选择是计算 **K/B** 的余数（K表示键值，B表示桶的数量）;
- 2、当键为字符串时，可以把每个字符看作一个整数，然后把它们累加起来，并将和处于B,然后取余数;

总的来说就是：一个桶可以包含多个存储块，并且每一个桶还有附加块以存放更多记录。而一个存储块（逻辑概念，不是实际存在的）是可以存放多个记录的。

- 对于散列表的插入，如果根据散列函数得出的桶有空闲空间，则将该记录直接放入桶中；如果对应的桶没有空闲空间，我们则需要增加一个溢出块到该桶的链上，并把新记录存入该块；
- 对于散列表的删除，大致上是通过散列函数找到对应的桶，并从桶内的存储块或者溢出块删除对应的记录。如果此时有溢出块，并且删除的记录是在桶内的存储块中，删除之后由于桶内存储有多余的空间，我们可以把溢出块的记录移动到存储块中。如果此时溢出块不包含记录，则可以删除溢出块。

从效率上来看，如果有足够多的桶，那么一般的查找只需要一次磁盘I/O就可以了。当文件不断增长，那么桶内就不能存储所有的记录。而记录的查找就转移到了链表，那么在链表中的每一个块都需要一次磁盘I/O。

因此相对于上面的静态散列，下面我们来看看两种动态散列：**可扩展散列、线性散列**；



## 动态哈希表

### 可扩展哈希

可扩展散列（Extendible Hashing）是一种动态散列技术，适用于文件系统、数据库等需要高效动态调整的应用场景。它是一种解决哈希冲突的方法，尤其适合处理数据量动态变化的情况。

算法思路就是，将key 哈希成二进制数据，二进制长度为K，然后取二进制数据序列第一位或最后一位算起的中的i位，这个i小于K。也就是说桶的数量为 2的i次方个，但不是直接哈希之后的二进制位就直接确认了桶的位置，而是通过一个目录来维护，当hash扩展时就通过扩展桶的数量，更新目录来维护哈希分布。

**关键概念：**

- 目录（Directory）：存储指向桶（bucket）的指针，每个指针对应一个桶。

- 桶（Bucket）：存储实际的数据条目，每个桶有固定的容量。

- 全局深度（Global Depth）：目录中二进制位的数量，决定了目录的大小。
- `globalDepth` 表示目录的深度，用来决定整个哈希表的哈希值中有多少位是有效的。
- 当整个哈希表的目录需要扩展时，`globalDepth` 会增加，目录大小翻倍。

- 局部深度（Local Depth）：每个桶中二进制位的数量，**注意：局部深度并不是桶的大小，而是桶的局部深度与桶的分裂操作相关。桶的大小根据情况制定，一般可以根据实际的磁盘的块大小和存储的数据之比来决定，这样在读取时能将整块数据一并读取出来**。
- `localDepth` 表示每个桶的深度，用来决定这个桶的哈希值中有多少位是有效的。它与桶的分裂操作相关。
- 当一个桶被分裂时，`localDepth` 会增加，以反映该桶的新的分裂状态。例如，如果一个桶的 `localDepth` 为 2，那么该桶对应的哈希值的前两位是相同的。
- `localDepth`（局部深度）并不是所有桶都相同。每个桶可以有不同的 `localDepth`，因为桶在不同时间点上可能经历过不同次数的分裂。

> 局部深度 (`localDepth`) 和全局深度 (`globalDepth`) 是可扩展散列中用来管理哈希表的结构信息。

**工作机制：**

- 当插入数据时，使用哈希函数计算数据的哈希值，并取全局深度对应的前几位作为目录索引。
- 如果目标桶已满，检查桶的局部深度是否小于全局深度：
- 如果是，则扩展该桶，增加局部深度，并分裂桶中的数据。
- 如果否，则增加全局深度，扩展目录，并重新分配所有桶。

**优点**

1. **动态扩展**：根据需要动态扩展，不需要预先分配大量空间。
2. **高效查找**：通过目录索引直接访问桶，提高查找效率。
3. **减少重散列**：避免了传统哈希表的全表重散列，减少性能开销。

**缺点**

1. **额外空间**：需要维护目录结构，占用额外空间。
2. **实现复杂**：比传统哈希表实现更复杂。



假设初始全局深度为2，目录和桶初始状态如下：

```less
目录：
00 -> Bucket 1
01 -> Bucket 2
10 -> Bucket 3
11 -> Bucket 4

桶：
Bucket 1: [ ]
Bucket 2: [ ]
Bucket 3: [ ]
Bucket 4: [ ]
```

我们假设每个桶最多能容纳2个元素。

#### 插入操作

1. 插入一个键"101": 计算哈希值，取前2位为“10”，将其存入Bucket 3。

```less
目录：
00 -> Bucket 1
01 -> Bucket 2
10 -> Bucket 3 (contains 101)
11 -> Bucket 4

桶：
Bucket 1: [ ]
Bucket 2: [ ]
Bucket 3: [101]
Bucket 4: [ ]
```

2. 插入另一个键"110": 计算哈希值，取前2位为“11”，将其存入Bucket 4。

```less
目录：
00 -> Bucket 1
01 -> Bucket 2
10 -> Bucket 3 (contains 101)
11 -> Bucket 4 (contains 110)

桶：
Bucket 1: [ ]
Bucket 2: [ ]
Bucket 3: [101]
Bucket 4: [110]
```

3. 插入键"011": 计算哈希值，取前2位为“01”，将其存入Bucket 2。

```less
目录：
00 -> Bucket 1
01 -> Bucket 2 (contains 011)
10 -> Bucket 3 (contains 101)
11 -> Bucket 4 (contains 110)

桶：
Bucket 1: [ ]
Bucket 2: [011]
Bucket 3: [101]
Bucket 4: [110]
```

3. 插入键"010": 计算哈希值，取前2位为“01”，将其存入Bucket 2，因为Bucket 2有空间。

```less
目录：
00 -> Bucket 1
01 -> Bucket 2 (contains 011, 010)
10 -> Bucket 3 (contains 101)
11 -> Bucket 4 (contains 110)

桶：
Bucket 1: [ ]
Bucket 2: [011, 010]
Bucket 3: [101]
Bucket 4: [110]
```

4. 插入键"111":

- 计算哈希值，取前2位为“11”，将其存入Bucket 4。Bucket 4已满，需要处理冲突。

```less
目录：
00 -> Bucket 1
01 -> Bucket 2 (contains 011, 010)
10 -> Bucket 3 (contains 101)
11 -> Bucket 4 (contains 110, 111) (满了)

桶：
Bucket 1: [ ]
Bucket 2: [011, 010]
Bucket 3: [101]
Bucket 4: [110, 111] (满了)
```

#### 处理冲突

因为Bucket 4已满，我们需要扩展目录。

1. 扩展全局深度：当前全局深度为2，扩展为3，目录大小加倍。**注意此处只是将深度扩展，但并未实际扩展桶，还是原来的几个桶**

```less
目录：
000 -> Bucket 1
001 -> Bucket 2
010 -> Bucket 3
011 -> Bucket 4
100 -> Bucket 1
101 -> Bucket 2
110 -> Bucket 3
111 -> Bucket 4
```

2. 重新分配数据：分裂Bucket 4，并重新分配其数据。**注意：这里只分裂Bucket 4，其他的桶仍然不变，也就是只增加了一个桶**

假设重新分配后，数据分布如下：

```less
目录：
000 -> Bucket 1
001 -> Bucket 2
010 -> Bucket 3
011 -> Bucket 4 (contains 011)
100 -> Bucket 1
101 -> Bucket 2
110 -> Bucket 3
111 -> New Bucket (contains 110, 111)

桶：
Bucket 1: [ ]
Bucket 2: [010]
Bucket 3: [101]
Bucket 4: [011]
New Bucket: [110, 111]
```

处理冲突中有几个关键点：

- 优先扩展目录，也就是桶的数量：这样只扩展那些已经满了的桶，而不需要扩展所有桶。这种局部扩展的特性使得系统能够更有效地管理内存，避免了不必要的资源浪费。
- 避免全局重分配：当某个桶满了时，只需要分裂这个桶并重新分配其内容，而不需要对整个哈希表进行全局重分配。这大大减少了重分配操作的开销，提升了系统的性能。比如上面的
- 不管是否扩展，后续查找时都是按照目录来先查找到Bucket，再从bucket中获取数据。



#### 代码实现

```java
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

/**
 * 扩展哈希
 *
 * @author liufei
 * @date 2024/7/1
 **/
public class ExtendibleHashing<K, V> {
    private       int                globalDepth;
    private       List<Bucket<K, V>> directory;
    private final int                bucketSize;

    public ExtendibleHashing(int bucketSize) {
        this.globalDepth = 1;
        this.bucketSize = bucketSize;
        this.directory = new ArrayList<>();
        // 这里 globalDepth 比 bucket.localDepth 大1，因为insert时会判断bucket.localDepth == globalDepth时 doubleDirectory
        this.directory.add(new Bucket<>(0));
        this.directory.add(new Bucket<>(0));
    }

    public V search(K key) {
        int hash = key.hashCode();
        int directoryIndex = hash & ((1 << globalDepth) - 1);
        Bucket<K, V> bucket = directory.get(directoryIndex);

        for (Map.Entry<K, V> entry : bucket.entries) {
            if (entry.getKey().equals(key)) {
                return entry.getValue();
            }
        }
        // 没有找到对应的键值对
        return null;
    }


    /**
     * hash运算：
     * 1 << globalDepth 表示将 1 左移 globalDepth 位，结果是一个具有 globalDepth 个二进制位的值，例如 globalDepth 为 2 时，结果是 100（即 4）。
     * (1 << globalDepth) - 1 表示该值减去 1，结果是一个所有低位比特都为 1 的掩码，例如 globalDepth 为 2 时，结果是 011（即 3）。
     * hash & ((1 << globalDepth) - 1) 表示将哈希值的低 globalDepth 位提取出来，用于确定目录中的位置。
     * &且，有0为0，比如：1100&1010-->1000
     *
     * @param key
     * @param value
     */
    public void insert(K key, V value) {
        int hash = key.hashCode();
        //？
        int directoryIndex = hash & ((1 << globalDepth) - 1);
        Bucket<K, V> bucket = directory.get(directoryIndex);

        if (bucket.isFull()) {
            if (bucket.localDepth == globalDepth) {
                doubleDirectory();
            }
            splitBucket(directoryIndex);
            insert(key, value);
        } else {
            bucket.insert(key, value);
        }
    }

    private void doubleDirectory() {
        List<Bucket<K, V>> newDirectory = new ArrayList<>(directory);
        newDirectory.addAll(directory);
        directory = newDirectory;
        globalDepth++;
    }

    private void splitBucket(int directoryIndex) {
        Bucket<K, V> bucket = directory.get(directoryIndex);
        // ？
        Bucket<K, V> newBucket = new Bucket<>(bucket.localDepth + 1);

        List<Map.Entry<K, V>> entries = new ArrayList<>(bucket.entries);
        bucket.clear();
        bucket.localDepth++;

        // 此处将原有的元素，重新散列到原有的bucket或者新的bucket中，此mask就是新的bucket的散列值，因为已经执行了bucket.localDepth++;
        int mask = 1 << (bucket.localDepth - 1);
        for (Map.Entry<K, V> entry : entries) {
            // newDirectoryIndex是原来的bucket对应的hash值
            int newDirectoryIndex = entry.getKey().hashCode() & ((1 << globalDepth) - 1);
            // 新的bucket的hash值 newDirectoryIndex 和原来的哈希值mask位与运算，如果==0，则说明不相等，插入原来的bucket中
            if ((newDirectoryIndex & mask) == 0) {
                bucket.insert(entry.getKey(), entry.getValue());
            } else {
                newBucket.insert(entry.getKey(), entry.getValue());
            }
        }

        for (int i = 0; i < directory.size(); i++) {
            // (i & mask) != 0 就是对应的新桶插入的位置，directory扩展之后，在不同的位置原来都是相同的bucket所以首先directory.get(i) == bucket成立
            if (directory.get(i) == bucket && (i & mask) != 0) {
                directory.set(i, newBucket);
            }
        }
    }

    private static class Bucket<K, V> {
        // localDepth 并不是所有桶都相同，因为桶在不同时间点上可能经历过不同次数的分裂。
        private       int                   localDepth;
        private final List<Map.Entry<K, V>> entries;

        public Bucket(int localDepth) {
            this.localDepth = localDepth;
            this.entries = new ArrayList<>();
        }

        public void insert(K key, V value) {
            entries.add(new HashMap.SimpleEntry<>(key, value));
        }

        public boolean isFull() {
            // 具体的大小需要根据实际的情况确认
            return entries.size() >= 4;
        }

        public void clear() {
            entries.clear();
        }
    }

    public static void main(String[] args) {
        ExtendibleHashing<Integer, String> hashing = new ExtendibleHashing<>(4);
        hashing.insert(1, "One");
        hashing.insert(2, "Two");
        hashing.insert(3, "Three");
        hashing.insert(4, "Four");
        hashing.insert(5, "Five");
    }
}
```



### 线性哈希

线性散列（Linear Hashing）是一种动态哈希技术，用于在数据库和文件系统中实现哈希表的扩展和缩减。其主要特点是可以在无需重新哈希全部数据的情况下，平滑地调整哈希表的大小，从而有效地处理数据量的增加或减少。

**关键点：哈希函数直接就是取模，维护一个哈希指针n，这个n就是桶的数量，并且维护一个正在分裂的桶的位置即变量p，然后将桶的增加和分裂的位置，与插入元素时桶的位置作分离。运行桶的元素数量超过桶的容量，允许数量溢出，在后续的插入中去通过分裂桶的过程中平衡桶中元素的数量。**

#### 代码实现

```java
import java.util.ArrayList;
import java.util.List;

public class LinearHashing<K, V> {

    private              List<List<Entry<K, V>>> buckets;
    // 每个桶的容量
    private              int                     bucketCapacity;
    // 最大使用的哈希函数的指针，即有多少个桶
    private              int                     n;
    // 当前正在分裂的桶变量
    private              int                     p;
    // 维护一个元素总数的变量
    private              int                     size;
    // 定义负载因子为元素总数与桶总数的比值。当负载因子低于一定阈值时，触发缩减操作。
    private static final double                  LOAD_FACTOR_THRESHOLD = 0.25;

    static class Entry<K, V> {
        K key;
        V value;

        Entry(K key, V value) {
            this.key = key;
            this.value = value;
        }
    }

    public LinearHashing(int initialBuckets, int bucketCapacity) {
        this.buckets = new ArrayList<>();
        for (int i = 0; i < initialBuckets; i++) {
            buckets.add(new ArrayList<>());
        }
        this.bucketCapacity = bucketCapacity;
        this.n = initialBuckets;
        this.p = 0;
        this.size = 0;
    }

    /**
     * 初始hash函数
     * 注意此hash函数是很重要的，如果hash函数改变了，比如不是取模，则分裂的方法就需要调整
     *
     * @param key key
     * @return 桶的位置
     */
    private int hash(K key) {
        return key.hashCode() % n;
    }

    /**
     * 扩展hash函数
     * 注意此hash函数是很重要的，如果hash函数改变了，比如不是取模，则分裂的方法就需要调整
     *
     * @param key key
     * @return 桶的位置
     */
    private int hash2(K key) {
        return key.hashCode() % (2 * n);
    }

    public void insert(K key, V value) {
        int bucketIndex = hash(key) >= p ? hash(key) : hash2(key);

        buckets.get(bucketIndex).add(new Entry<>(key, value));
        size++;

        /*
         * 当添加的元素大于了其容量就触发一次分裂，此时buckets中的元素是存在短暂的大于其容量的，但超过的大小不会超过桶的数量n，
         * 因为每次大于容量bucketCapacity都会触发分裂，分裂都是从分裂点开始的，
         * 极端情况下n次都是插入到此桶，那也必定会触发当前超过的桶的分裂，因为n就是将所有的桶都分裂了一遍
         */
        if (buckets.get(bucketIndex).size() > bucketCapacity) {
            split();
        }
    }

    private void split() {

        List<Entry<K, V>> toRemove = new ArrayList<>();
        List<Entry<K, V>> newBucket = new ArrayList<>();

        // 调整已有桶中的元素
        List<Entry<K, V>> oldBucket = buckets.get(p);
        for (Entry<K, V> entry : oldBucket) {
            int newIndex = hash2(entry.key);
            if (newIndex != p) {
                newBucket.add(entry);
                toRemove.add(entry);
            }
        }
        oldBucket.removeAll(toRemove);

        /*
         * 这里直接 buckets.add(newBucket) 即可，原因在于hash函数是直接取模，分裂是按照顺序往下分裂的
         * 那分裂的原来的元素，必定就在要么是原来的bucket中，要么就在这个新增的bucket中
         * 比如：初始4个桶- 0、1、2、3，在经过了一次分裂后，此时再分裂桶1，则此时的桶的情况必然是 0、1、2、3、4
         * 增加一个桶就是桶5，桶的情况是0、1、2、3、4、5，此时新的元素，则必然在原来的桶1和新的桶5中
         * 比如key为5则原来取模4是桶1，现在取模8就是桶5，key为9则原来取模4是桶1，现在取模8还是桶1
         */
        buckets.add(newBucket);

        p += 1;
        if (p == n) {
            p = 0;
            n *= 2;
        }
    }

    public V search(K key) {
        int bucketIndex = hash(key) >= p ? hash(key) : hash2(key);
        if (bucketIndex < buckets.size()) {
            for (Entry<K, V> entry : buckets.get(bucketIndex)) {
                if (entry.key.equals(key)) {
                    return entry.value;
                }
            }
        }
        return null;
    }

    public void delete(K key) {
        int bucketIndex = hash(key) >= p ? hash(key) : hash2(key);
        if (bucketIndex < buckets.size()) {
            if (buckets.get(bucketIndex).removeIf(entry -> entry.key.equals(key))) {
                size--;
                // Check if we need to merge buckets
                if ((double) size / buckets.size() < LOAD_FACTOR_THRESHOLD) {
                    merge();
                }
            }
        }
    }

    /**
     * 删除之后，缩减哈希表
     * 将部分桶合并，并重新计算这些桶中的元素哈希值
     */
    private void merge() {
        if (n <= 1) return; // Can't merge any further

        // Merge the last bucket back into the hash table
        int mergeIndex = n + p - 1;
        List<Entry<K, V>> mergedBucket = buckets.get(mergeIndex);

        for (Entry<K, V> entry : mergedBucket) {
            int bucketIndex = hash(entry.key);
            if (bucketIndex >= buckets.size()) {
                bucketIndex = hash2(entry.key);
            }
            buckets.get(bucketIndex).add(entry);
        }

        buckets.remove(mergeIndex);

        p -= 1;
        if (p < 0) {
            p = n - 1;
            n /= 2;
        }
    }

    public void printBuckets() {
        for (int i = 0; i < buckets.size(); i++) {
            System.out.print("Bucket " + i + ": ");
            for (Entry<K, V> entry : buckets.get(i)) {
                System.out.print("[" + entry.key + " : " + entry.value + "] ");
            }
            System.out.println();
        }
    }

    public static void main(String[] args) {
        LinearHashing<Integer, String> lh = new LinearHashing<>(4, 2);

        // Insert keys and values
        int[] keys = {12, 23, 34, 45, 56, 67, 78, 89};
        String[] values = {"A", "B", "C", "D", "E", "F", "G", "H"};
        for (int i = 0; i < keys.length; i++) {
            lh.insert(keys[i], values[i]);
            System.out.println("Inserted " + keys[i] + " : " + values[i]);
            lh.printBuckets();
        }

        // Search keys
        for (int key : keys) {
            String value = lh.search(key);
            System.out.println("Key " + key + " found: " + value);
        }

        // Delete keys
        for (int key : keys) {
            lh.delete(key);
            System.out.println("Deleted " + key);
            lh.printBuckets();
        }
    }
}

```



## 一致性哈希

一致性哈希（Consistent Hashing）是一种分布式系统中用于将请求映射到物理节点（如服务器、缓存节点等）的方法。它解决了在分布式环境中节点动态加入或退出时如何有效地分配和重新分配键的问题。与传统的哈希算法相比，一致性哈希能够最小化数据的重新分布，减少系统开销和中断。

#### 基本原理

1. **哈希环**：一致性哈希将哈希空间组织成一个虚拟的环（通常是一个0到2^32-1的整数环）。环上的每个点表示一个哈希值。
2. **节点映射**：每个物理节点通过哈希函数被映射到环上的一个或多个点。通常一个节点会被映射到多个点（称为虚拟节点）以实现负载均衡。
3. **数据映射**：数据项也通过哈希函数被映射到环上的点。每个数据项会被存储在顺时针方向遇到的第一个节点上。
4. **节点增加**：当有新的节点加入时，只需要将受影响的数据重新分配到新节点上，其他数据不受影响。
5. **节点移除**：当节点移除时，只需将该节点上的数据重新分配到下一个顺时针方向的节点上，其他数据同样不受影响。

一致性哈希的优势：

1. **最小化数据移动**：新节点的加入或节点的失效只影响到环上的相邻节点，其他节点的数据无需移动。
2. **扩展性**：系统中添加或移除节点时，系统负载可以自动平衡。
3. **容错性**：由于每个数据项可以有多个副本（在不同的节点上），即使有节点故障，数据也可以从其他副本恢复。



#### 示例

比如对每个节点分别设置 3 个虚拟节点：

- 对节点 A 加上编号来作为虚拟节点：A-01、A-02、A-03
- 对节点 B 加上编号来作为虚拟节点：B-01、B-02、B-03
- 对节点 C 加上编号来作为虚拟节点：C-01、C-02、C-03

引入虚拟节点后，原本哈希环上只有 3 个节点的情况，就会变成有 9 个虚拟节点映射到哈希环上，哈希环上的节点数量多了 3 倍。

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/data-structure/hashloop.png" alt="image" style={{ maxWidth: '36%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>

[//]: # (<img src="/Users/liufei/docs/typora笔记/images/hashloop.png" style="zoom:30%;" />)



可以看到，节点数量多了后，节点在哈希环上的分布就相对均匀了。这时候，如果有访问请求寻址到「A-01」这个虚拟节点，接着再通过「A-01」虚拟节点找到真实节点 A，这样请求就能访问到真实节点 A 了。

上面为了方便理解，每个真实节点仅包含 3 个虚拟节点，这样能起到的均衡效果其实很有限。而在实际的工程中，虚拟节点的数量会大很多，比如 Nginx 的一致性哈希算法，每个权重为 1 的真实节点就含有160 个虚拟节点。

另外，虚拟节点除了会提高节点的均衡度，还会提高系统的稳定性。当节点变化时，会有不同的节点共同分担系统的变化，因此稳定性更高。

比如，当某个节点被移除时，对应该节点的多个虚拟节点均会移除，而这些虚拟节点按顺时针方向的下一个虚拟节点，可能会对应不同的真实节点，即这些不同的真实节点共同分担了节点变化导致的压力。

而且，有了虚拟节点后，还可以为硬件配置更好的节点增加权重，比如对权重更高的节点增加更多的虚拟机节点即可。



#### 代码实现

```java
import java.util.SortedMap;
import java.util.TreeMap;

/**
 * 一致性哈希
 *
 * @author liufei
 * @date 2024/7/2
 **/
public class ConsistentHashing {

    // 映射的哈希环
    private final SortedMap<Integer, String> ring = new TreeMap<>();
    // 虚拟节点的数量
    private final int                        numberOfReplicas;

    public ConsistentHashing(int numberOfReplicas, String[] nodes) {
        this.numberOfReplicas = numberOfReplicas;
        for (String node : nodes) {
            addNode(node);
        }
    }

    public void addNode(String node) {
        for (int i = 0; i < numberOfReplicas; i++) {
            ring.put((node + i).hashCode(), node);
        }
    }

    public void removeNode(String node) {
        for (int i = 0; i < numberOfReplicas; i++) {
            ring.remove((node + i).hashCode());
        }
    }

    public String getNode(String key) {
        if (ring.isEmpty()) {
            return null;
        }
        int hash = key.hashCode();
        if (!ring.containsKey(hash)) {
            SortedMap<Integer, String> tailMap = ring.tailMap(hash);
            hash = tailMap.isEmpty() ? ring.firstKey() : tailMap.firstKey();
        }
        return ring.get(hash);
    }

    public static void main(String[] args) {
        String[] nodes = {"A", "B", "C", "D"};
        ConsistentHashing ch = new ConsistentHashing(3, nodes);

        System.out.println("Node for key 'myKey': " + ch.getNode("myKey"));
        System.out.println("Node for key 'anotherKey': " + ch.getNode("anotherKey"));

        ch.addNode("E");
        System.out.println("Node for key 'myKey' after adding node E: " + ch.getNode("myKey"));

        ch.removeNode("B");
        System.out.println("Node for key 'myKey' after removing node B: " + ch.getNode("myKey"));
    }
}

```

