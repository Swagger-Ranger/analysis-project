---
title: Kafka手册3-Streams
sidebar_position: 3
toc_min_heading_level: 2
toc_max_heading_level: 5
---

### 为什么需要Kafka Streams

为什么需要`Kafka Streams`，是我需要一种类似于`Esper`的复杂事件处理（CEP）引擎，用于实时分析事件流，识别事件模式，提供事件驱动的响应。而 Esper 感觉用的人少，社区不够友好，不如Kafka社区强大。而且Kafka streams 天然具备和Kafka无缝集成的优势，也就是说如果同样适用Kafka来作为消息中间件，那使用 Kafka streams 可以直接写聚合和分析逻辑，并且具备如下的优点：

- **Kafka集成，无服务架构**：Kafka Streams 是 Kafka 的一个组件，提供了一个轻量级的、分布式的库直接与 Kafka 集成，无需额外的集群或服务，使用 Kafka 作为数据输入和输出源。可以嵌入到任何标准的 Java 应用程序中，不需要独立的处理集群。
- **编程模型**：简洁的 API，可以用来定义流处理的拓扑结构，支持状态存储、窗口操作、聚合、连接等功能。
- **容错和扩展性**：通过 Kafka 的分区机制和消费者组，Kafka Streams 提供了内建的容错和扩展性。
- **事件处理模型**：支持事件驱动的编程模型，可以处理无界的数据流。

所以当需要在 Kafka 环境中进行实时流处理的应用，如日志分析、实时监控、ETL 流水线等，就可以使用 Kafka Streams。



### 流处理概念

Kafka Streams的流处理概念，有拓扑、时间、状态、时间窗口、流和表等概念。

#### 拓扑

在 Kafka Streams 中，**拓扑（topology）** 是指流处理应用程序中所有处理步骤（如源、转换、聚合、分支和接收器）的有向无环图 (DAG)。拓扑定义了数据从源节点（如 Kafka 主题）到接收器节点（如 Kafka 主题或外部存储）的流动路径和处理逻辑。（流处理器有过滤器、计数、分组和左连接等）。



#### 流和表

- 表：表是记录的集合，每条记录都有一个主键标识，并包含了一组由模式定义的 属性。表的记录是可变的(可以执行更新和删除操作)。可以通过查询表获知数据在某一时刻的状态。例如，通过查询某客户表`CUSTOMERS_CONTACTS`，就可以获取所有客户当前的联系信息。如果表中不包含历史数据，那么就找不到客户过去的联系信息。

- 流：与表不同，流包含了历史变更数据。 是一系列事件，每个事件就是一个变更。表表示的是世界的当前状 态，是发生多个变更后的结果。可见，表和流是同一枚硬币的两面——世界总是在发生变化，我们有时候对导致发生变化的事件感兴趣，有时候对世界的当前状态感兴趣。如果一个系统允许通过这两种方式来看待数据，那么它就比只支持一种方式的系统更强大。

要将表转化成流，需要捕获所有对表做出的变更。要将 insert 事件、update 事件和 delete 事件保存 到流里。大多数数据库提供了 CDC 解决方案，有很多Kafka连接器可以将这些变更发送到 Kafka，用于后续的流式处理。

要将流转化成表，需要应用流里所有的变更。这也叫作流的`物化`*（`物化`这个概念就是将一系列变动固定下来，在mysql的一些聚合查询中也常常使用叫物化视图的概念，即将一系列数据查询结果或者变动数据先保存下来，后续查询时以加快查询速度）*。我们需要在内存、内部状态存储或外部数据库中创建一张表，然后从头到尾遍历流里所有的事件，逐个修改状态。在完成这个过程之后，就得到了一张表，它代表了某个时间点的状态。



#### 时间

时间可能是流式处理中最为重要的概念，也是最让人感到困惑的概念。在流式处理中，形成一个通用的时间概念非 常重要，因为大部分流式应用程序的操作是基于时间窗口的。例如，我们可能有一个计算股价 5 分钟移动平均数的流式应用程序。如果一个生产者因为网络问题离线 2 小时，并在重新连线后返回 2 小时的数据， 那么我们就需要知道该如何处理这些数据。这些数据大多与过去了很久的 5 分钟时间窗口有关，而且已经计算并保存结果了。

流式处理系统一般包含以下几种时间：

- 事件时间：指事件的发生时间和消息的创建时间，比如指标的生成时间、商店里商品的出售时间、网站用户访问网页的时间，等等。在 Kafka 0.10.0 和更高版本中，生产者会自动在消息里添加消息的创建时间。如果这个时间戳与应用程序对 的定义不一样(例如，Kafka 消息是在事件发生以后基于数据库记录而创建的)，那么建议将事件时间作为一个单独的字段添加到消息里，这样在后续处理事件时两个时间戳将都可用。在处理数据流时，事件时间是非常重要的。

- 日志追加时间：指事件到达并保存到 broker 的时间，也叫摄取时间。在 Kafka 0.10.0 和更高版本中， 如果启用了自动添加时间戳的功能，或者记录是用旧版本生产者客户端生成的，并且不包含时间戳，那么 broker 就会在收到记录时自动添加时间戳。这个时间戳通常与流式处理没有太大关系，因为一般只对 事件的发生时间感兴趣。如果要计算每天生产了多少台设备，就需要计算在那一天实际生产的设备数量， 尽管这些事件有可能因为网络问题第二天才进入Kafka。不过，如果事件时间没有被记录下来，则也可以考虑使用日志追加时间，因为它在记录创建之后就不会发生变化，而且如果事件在数据管道中没有延迟， 那么就可以将其作为事件发生的近似时间。
- 处理时间：指应用程序在收到事件之后要对其进行处理的时间。这个时间可以是在事件发生之后的几毫秒、几小时或几天。同一个事件可能会被分配不同的处理时间戳，具体取决于应用程序何时读取这个事件。即使是同一个应用程序中的两个处理线程，它们为事件分配的时间戳也可能不一样。所以，这个时间戳非常不可靠，最好避免使用它。

>  Kafka Streams 基于 TimestampExtractor 接口为事件分配时间。Streams 应用程序开发人员可以使用这个接 口的不同实现，既可以使用前面介绍的 3 种时间语义，也可以使用完全不同的时间戳，包括从事件中提取 的时间戳。

当 Streams 将输出的消息写入 Kafka 主题时，它会根据以下规则为每个事件分配时间戳：

- 如果输出记录直接映射到输入记录，那么将使用输入记录的时间戳作为输出记录的时间戳。
- 如果输出记录是一个聚合结果，那么将使用聚合中最大的时间戳作为输出记录的时间戳。
- 如果输出记录是两个流的连接结果，那么将使用被连接的两个记录中较大的时间戳作为输出记录的时间戳。
- 如果连接的是一个流和一张表，那么将使用流记录的时间戳作为输出记录的时间戳。
- 如果输出记录是由 Streams 函数(如 punctuate())生成的，该函数会在一个特定的时间调度内生 成数据，而不管输入是什么，那么输出记录的时间戳将取决于流式处理应用程序的当前内部时间。
- 如果开发人员使用的是 Streams 底层处理 API 而不是 DSL，那么就可以使用直接操作记录时间戳的 API， 从而实现符合应用程序业务逻辑的时间戳语义。



#### 时间窗口

大部分针对流的操作是基于时间窗口的，比如移动平均数、一周内销量最好的产品、99 百分位系统负 载，等等。两个流的连接操作也是基于时间窗口的——我们会连接发生在相同时间片段内的事件。但是， 很少会有人停下来仔细想想他们需要哪些时间窗口。在处理时间数据分析例如计算移动平均数时，需要要知道如下信息：

- 窗口大小

计算5分钟内，还是 15 分钟，抑或一天的平均数 ? 窗口越大就越平滑，但滞后也越多。如果价格涨了，则需要更长的时间才能看出来。Streams 提供了一种 ，其大小是通过不活跃的时间段来定义的。开发人员会定义一个会话间隙，所有连续到达且间隙小于这个会话间隙的事件都属于同一个会话。一个大的间隙将开始一个新会话，在这个间隙之后但在下一个间隙之前到达的所有事件都属于这个会话。

- 窗口移动频率（移动间隔）

5 分钟平均数可以每分钟或每秒变化一次，或者在有新事件到达时发生变化。时间间隔固定的窗口叫作`跳跃窗口(hopping window)`，移动间隔与窗口大小相等的窗口叫作`滚动窗口(tumbling window)`。

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/kafka/kafka-23.png" alt="image" style={{ maxWidth: '39%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>

[//]: # (<img src="/Users/liufei/docs/typora笔记/images/kafka/kafka-23.png" alt="image" style="zoom:39%;" />)

- 窗口可以更新时间（宽限期）

假设我们已经计算出了 00:00 和 00:05 之间的 5 分钟移动平均数，一小时后，又收到了一些为 00:02 的事件，那么需要更新 00:00~00:05 这个窗口的结果吗?或者就这么算了？理想情况下，可以定义一个时间段，在这个时间段内，事件可以被添加到与它们对应的时间片段里。可以规定如果事件延迟不 超过 4 小时，就重新计算并更新结果，否则就忽略它们。



#### 处理保证

无论是否出现故障，都能够一次且仅一次处理每一条记录，这是流式处理应用程序的一个关键需求。如果没有精确一次性保证，那么流式处理就不能被用在要求精确结果的场景中。**Kafka 支持精确一次性语义。Kafka 生产者支持事务和幂等性。Streams 借助 Kafka 的事务特性为流式处理 应用程序提供精确一次性保证。**使用 Streams 库的应用程序可以将 `processing.guarantee` 设置为 `exactly_once`，以此来启用精确一次性保证。Streams 2.6 或更高版本提供了更高效的精确一次性实 现，它需要 broker 2.5 或更高版本。可以将 `processing.guarantee` 设置为 `exactly_once_beta`， 以此来启用这种更高效的实现。





### Kafka Streams 关键特性

#### 单消费者多分区合并

Kafka Streams 在单消费者中提供了类似于 MapReduce 的数据处理能力，它能够在自己分配到的多个分区上并行处理数据并自动合并结果。Kafka Streams 框架提供了多种操作符，可以用来对分区的数据进行操作，并在必要时将结果合并。这些操作符包括 map、filter、groupByKey、reduce、aggregate 等。

**注意：如果有多个消费者实例（也就是 Kafka Streams 应用程序运行在多个实例上），跨消费者实例的结果合并需要额外的处理。因为每个消费者实例只处理自己分配到的分区的数据。**



#### 流与流的连接

需要连接两个真实的事件流，而不是一个流和一张表， 流是无边界的。如果用一个流来表示一张表，那么就可以忽略流的大部分历史事件，因为只关心表的当前状态。但是，如果要连接两个流，则要连接所有的历史事件，也就是将两个流里具有相同键和发生在相同时间窗口内的事件匹配起来。这就是为什么流和流的连接也作**基于时间窗口的连接** 。

假设我们有一个用户搜索事件流和一个用户点击搜索结果事件流。我们想要匹配用户的搜索和用户对搜索 结果的点击，以便知道哪个搜索的热度更高。很显然，我们需要基于搜索关键词进行匹配，而且只能匹配 一个时间窗口内的事件。假设用户会在输入搜索关键词几秒之后点击搜索结果。因此，我们为每一个流保 留了几秒的时间窗口，并对每个时间窗口内的事件进行匹配，Streams 支持等价连接(equi-joins)，流、查询、点击事件都是通过相同的键来进行分区的，而且这些键就是连接用的键。这样一来，user_id:42 所有的点击事件都会被保存到点击主题的分区 5 中，而 user_id:42 所有的搜索事件都会被保存到搜索主题的分区 5 中。然后，Streams 会确保这两个主题的分 区 5 被分配给同一个任务，这样这个任务就可以看到所有与 user_id:42 相关的事件。Kafka Streams 在内嵌的`RocksDB`中维护了两个主题的连接时间窗口，所以能执行连接操作。

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/kafka/kafka-20.png" alt="image" style={{ maxWidth: '56%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>

[//]: # (<img src="/Users/liufei/docs/typora笔记/images/kafka/kafka-20.png" alt="image" style="zoom:50%;" />)



#### 流与表的连接

有时候流式处理需要将外部数据和流集成在一起，比如根据保存在外部数据库中的规则来验证事务，或者将用户信息填充到点击事件流中。要使用外部查找来实现数据填充，常规的是通过查询外部表的方式来填充数据再把填充好的数据用新的流发送出去。

但这个方式有个很严重的问题就是：外部查找会严重增加处理每条记录的延迟，通常为 5~15 毫秒。这在很多情况 下是不可行的。另外，给外部数据存储造成的额外负担也是不可接受的——流式处理系统每秒可以处理 10~50 万个事件，而数据库正常情况下每秒只能处理 1 万个事件。这也增加了可用性方面的复杂性，因为当外部存储不可用时，应用程序需要知道该作何处理。

为了获得更好的性能和伸缩性，需要在流式处理应用程序中缓存从数据库读取的信息。不过，管理缓存也是一个大问题。例如，该如何保证缓存中的数据是最新的?如果刷新太过频繁，则仍然会对数据库造成压力，缓存也就失去了应有的作用。如果刷新不及时，那么流式处理用的就是过时的数据。

如果能够捕获数据库变更事件，并形成事件流，那么就可以让流式处理作业监听事件流，然后根据变更事件及时更新缓存。捕获数据库变更事件并形成事件流的过程叫作 CDC（Change Data Capture），Connect 提供了一些连接器用于执行 CDC 任务，并把数据库表转成变更事件流。这样我们就拥有了数据库表的私有副本，一旦数据库发生变更，我们就会收到通知，并根据变更事件更新私有副本里的数据这样一来，每当收到点击事件，我们就从本地缓存里查找 user_id，并将其填充到点击事件中。因为使 用的是本地缓存，所以具有更强的伸缩性，而且不会影响数据库和其他使用数据库的应用程序。如下图：

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/kafka/kafka-21.png" alt="image" style={{ maxWidth: '39%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>

[//]: # (<img src="/Users/liufei/docs/typora笔记/images/kafka/kafka-21.png" alt="image" style="zoom:39%;" />)

之所以将这种方式叫作**流与表的连接**，是因为其中一个流代表了本地缓存表的变更。

**操作示例**

具体在Kafka streams中要实现数据cdc，主要是两块，一个是Kafka streams中的配置，一个是数据变更端的配置

```java
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.kstream.Consumed;
import org.apache.kafka.streams.kstream.KStream;

import java.util.Properties;
/**
 * @author liufei
 **/
public class CDCExample {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, "cdc-example");
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());

        // 使用 StreamsBuilder 构建流处理拓扑，处理 CDC 数据。
        StreamsBuilder builder = new StreamsBuilder();

        // 读取 CDC 事件流
        KStream<String, String> cdcStream = builder.stream("cdc-topic", Consumed.with(Serdes.String(), Serdes.String()));

        // 处理 CDC 事件
        cdcStream.foreach((key, value) -> {
            // 在这里处理 CDC 事件，例如：
            System.out.println("Received CDC event: key=" + key + ", value=" + value);
        });

        KafkaStreams streams = new KafkaStreams(builder.build(), props);
        streams.start();
        Runtime.getRuntime().addShutdownHook(new Thread(streams::close));
    }
}
```

数据端的使用CDC工具（如`Debezium`）捕获并写入 Kafka 主题，比如mysql的配置，`Debezium MySQL` 连接器配置示例

```json
{
    "name": "mysql-connector",
    "config": {
        "connector.class": "io.debezium.connector.mysql.MySqlConnector",
        "tasks.max": "1",
        "database.hostname": "localhost",
        "database.port": "3306",
        "database.user": "debezium",
        "database.password": "dbz",
        "database.server.id": "184054",
        "database.server.name": "dbserver1",
        "table.whitelist": "mydb.mytable",
        "database.history.kafka.bootstrap.servers": "localhost:9092",
        "database.history.kafka.topic": "schema-changes.mydb"
    }
}
```



#### 乱序事件

无论是流式处理系统还是传统的 ETL 系统，处理乱序事件对它们来说都是一个挑战。物联网领域经常出 现乱序事件，这也是意料之中的，如下图：

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/kafka/kafka-22.png" alt="image" style={{ maxWidth: '50%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>

[//]: # (<img src="/Users/liufei/docs/typora笔记/images/kafka/kafka-22.png" alt="image" style="zoom:50%;" />)

例如，一个移动设备断开 WiFi 连接几小时，在重新连 上后将几小时以来累积的事件一起发送出去。这种情况在监控网络设备时(发生故障的交换机在修复之前 不会发送任何诊断数据)或在生产车间(车间的网络连接非常不可靠)里也时有发生。

要让流处理应用程序处理好这些场景，需要做到以下几点：

- 识别乱序事件。应用程序需要检查事件的时间，并将其与当前时间对比。 规定一个时间段用于重排乱序事件。例如，3 小时以内的事件可以重排，但 3 周以外的事件可以直接丢弃。

- 能够带内(in-band)重排乱序事件。这是流式处理与批处理作业的一个主要不同点。如果我们有一个 每天运行的作业，一些事件在作业结束之后才到达，那么可以重新运行昨天的作业并更新事件。而在流式处理中，“重新运行昨天的作业”这种事情是不存在的，乱序事件和新到达的事件必须一起处理。

> 带内（in-band）重排乱序事件是指在同一个数据流或通信信道中，数据包或消息在传输过程中由于各种原因顺序发生变化。例如，在网络传输中，网络拥塞、路由变化或其他网络问题可能导致数据包到达目的地的顺序与发送时的顺序不一致。

- 能够更新结果。如果处理结果是保存到数据库中，那么可以通过 put 或 update 更新结果。如果处理结果是通过邮件发送的，则需要用到一些巧妙的方法。

有些流式处理框架(比如 Google Dataflow 和 Kafka Streams)支持处理独立于处理时间的事件，能够处理比当前时间更晚或更早的事件。它们在本地状态里维护了多个可更新的聚合时间窗口，为开发人员提供了配置时间窗口可更新时长的能力。*时间窗口可更新时间越长，维护本地状态需要的内存就越大*。

**Kafka Streams 提供了 `KStream` 和 `KTable` 等抽象，可以使用窗口操作（windowing）来处理乱序事件。**

> Streams API 通常会将聚合结果写到主题中。这些主题一般是压缩日志主题(compacted topics)，也就是说，它们只保留每个键的最新值。如果一个聚合时间窗口的结果因为晚到事件需要被更新，那么 Kafka Streams 会直接为这个聚合时间窗口写入一个新结果，将前一个覆盖掉。



##### Kafka streams内存重排序的安全性

在 Kafka Streams 中，内存中的重排序数据确实有丢失的风险，但 Kafka Streams 通过多种机制来减轻这个风险，并尽可能保证数据的可靠性和一致性：

- **状态存储（State Stores）**： Kafka Streams 使用状态存储来保存处理过程中需要持久化的数据。状态存储可以是内存中的，也可以是持久化到磁盘上的 RocksDB。对于重排序操作，Kafka Streams 会在状态存储中保存窗口内的数据。

```java
// 配置状态存储和窗口,使用 Stores.persistentWindowStore 方法来配置一个持久化的状态存储，这样即使应用程序宕机，数据也可以从磁盘上恢复。
        stream.groupByKey()
              .windowedBy(TimeWindows.of(Duration.ofMinutes(1)))
              .aggregate(
                  () -> "",
                  (key, value, aggregate) -> aggregate + value,
                  Materialized.<String, String, WindowStore<Bytes, byte[]>>as(
                      Stores.persistentWindowStore("reordering-store", Duration.ofHours(1), Duration.ofMinutes(1), false))
                  .withKeySerde(Serdes.String())
                  .withValueSerde(Serdes.String())
              )
              .toStream()
              .map((Windowed<String> key, String value) -> new KeyValue<>(key.key(), value))
              .to("output-topic");
  ```



- **日志压缩主题（Changelog Topics）**： 每个状态存储都会有一个对应的日志压缩主题（changelog topic），用于记录状态存储的变更。Kafka Streams 会将状态存储的变更异步地写入到这些主题中，以便在故障恢复时重建状态。

- **容错机制**： 当 Kafka Streams 应用程序发生故障并重启时，它会从日志压缩主题中读取数据，重建状态存储，从而恢复到故障发生前的状态。这样，即使内存中的数据丢失，状态存储的数据也可以从日志压缩主题中恢复。



### 示例

《Kafka 权威指南》作者提供的示例，我增加了代码阅读注释，地址如下：

[wordcount](https://github.com/Swagger-Ranger/kafka-streams-wordcount.git)

[stockstats](https://github.com/Swagger-Ranger/kafka-streams-stockstats.git)

Kafka streams 东西挺多的，todo。。。
