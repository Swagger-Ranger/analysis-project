---
title: Kafka手册2-架构
sidebar_position: 2
toc_min_heading_level: 2
toc_max_heading_level: 5
---

## Kafka架构实现



### 控制器

Kafka 控制器（Controller）是 Kafka 集群中一个非常重要的组件，负责集群元数据的管理和协调。它是由集群中的一个 broker 选举产生的，通常是 Kafka 集群中的第一个 broker。如果这个 broker 出现故障，集群会重新选举一个新的 broker 作为控制器。这部分就是一个分布式协调问题，新版本的控制器从zookeeper变成了Kafka自带的Kraft。



### 请求处理

Kafka的网络请求处理流程，无论是针对生产者、消费者还是管理请求，都是通过类似的多线程和队列机制来实现的。在 Kafka broker 中，处理客户端请求的过程涉及接收器线程（Acceptor Thread）、处理器线程（Processor Thread，也叫网络线程）和 I/O 线程（I/O Thread）。

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/kafka/kafka-12.png" alt="image" style={{ maxWidth: '90%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>

[//]: # (<img src="/Users/liufei/docs/typora笔记/images/kafka/kafka-12.png" style="zoom:90%;" />)

具体过程如下：

```css
Client ---> [Acceptor线程] ---> [SocketChannel] ---> [Processor线程] ---> [Request Queue请求队列] ---> [I/O线程] ---> [Response Queue响应队列] ---> [Processor线程] ---> Client
```

1. **连接建立**：生产者、消费者或者管理(如Kafka Manager)客户端，发送连接请求，并由`Acceptor`线程接受新的客户端连接与Kafka broker建立TCP长连接，并为每个连接创建一个`SocketChannel`，数据都通过`SocketChannel`传输。并交由`Processor`线程负责该连接的后续所有读写操作。

> **在连接建立之后，客户端通过SocketChannel与Broker进行通信。此时，新的请求和响应都不再需要经过Acceptor线程，而是直接通过Processor线程处理的SocketChannel来传递。**

2. **请求发送**：客户端通过建立的`SocketChannel`将消息发送到Kafka broker。

3. **请求接收**：Kafka broker的`Processor`线程接收请求，并放入请求队列`Request Queue`。

4. **请求处理**：I/O线程从请求队列中取出请求，处理消息（如写入磁盘）。

5. **响应发送**：处理完成后，将响应放入响应队列`Response Queue`。

6. **响应接收**：`Processor`线程从响应队列中取出响应，通过`SocketChannel`将响应发送回生产者。

> 请求和响应之间的匹配是通过请求的唯一标识符和客户端连接上下文来实现的。Request对象中就包含了SocketChannel和请求的唯一标识符，并放入Request Queue。然后Processor线程从Response Queue中取出Response对象，使用Response对象中的唯一标识符，找到对应的SocketChannel，最后通过SocketChannel发送响应数据给客户端。并且延迟的响应会被放在临时内存中，直到它们可以被发送给客户端。



#### 元数据请求

在集群环境下，生产请求和消费请求以及跟随者的数据同步请求，都必须发送给分区的首领。如果 broker 收到一个针对某个分区的写入请求，而这个分区的首领在另一个 broker 上，那么发送请求的客户端将收到“非分区首领”错误响应。如果针对某个分区的 读取请求被发送到一个不包含这个分区首领的 broker 上，那么也会收到同样的错误。Kafka 客户端负责把生产请求和获取请求发送到包含分区首领的 broker 上。

那么客户端怎么知道该向哪里发送请求呢?客户端使用了另一种请求类型，也就是 ，请求中包含了客户端感兴趣的主题清单。这种请求的响应消息里指明了这些主题所包含的分区、每个分区都有哪些 副本，以及哪个副本是首领。元数据请求可以被发送给任意一个 broker，因为所有broker都缓存了这些元数据信息。并且客户端会把这些信息缓存起来，并直接向目标 broker 发送生产请求和获取请求。但需要定时发送元数据请求来刷新缓存(刷新的时间间隔可以通过 `metadata.max.age.ms` 参数来配 置)，以便知道元数据是否发生了变化，如果客户端收到“非分区首领”错误，那么它也会在重新发送请求之前刷新元数 据，因为这个错误说明客户端正在使用过期的元数据。



#### 消费者请求消息

**消费者请求消息数据上限**

客户端发送请求，请求broker 返回指定主题、 分区和特定偏移量位置的消息，因为消费者请求的Kafka消息会被暂存在内存中，所以客户端可以指定一个分区最多可以返回多少数据，即请求返回的上限，这个上限限制非常重要，因为客户端需要为 broker 返回的数据分配足够的内存。如果没有这个限制，并且 broker 返回了大量的数据，则可能会耗尽客户端的内存。

**消费者请求消息数据下限**

除了可以设置 broker 返回数据的上限，客户端也可以设置 broker 返回数据的下限。在主题消息流量不是很大的情况 下，这样可以减少 CPU 和网络开销。具体操作如下:客户端发送一个请求，broker 在等到有足够数据时才把它们返回给客户端，然后客户端再发起另一个请求，而不是让客户端每隔几毫秒就发送一次请求，可能每次都只能拿到很少的数据，甚至没有数据，如下图。对比这两种情况，虽然它们最终读取的数 据总量是一样的，但前者的来回传送次数更少，开销也更小。

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/kafka/kafka-13.png" alt="image" style={{ maxWidth: '80%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>

[//]: # (<img src="/Users/liufei/docs/typora笔记/images/kafka/kafka-13.png" alt="image" style="zoom:80%;" />)



**消费者安全拉取消息**

不是所有保存在分区首领上的数据都可以被客户端读取。大部分客户端只能读取已经被写入所有同步副本 （ 跟随者副本除外(尽管它们也是消费者)，否则复制功能将无法正常工作 ）的消息。分 区首领知道哪些消息已经被复制到哪些副本上，所以消息在还没有被写入所有同步副本之前是不会被发送 给消费者的——尝试获取这些消息的请求会得到空响应，而不是错误。

之所以这样，是因为还没有被足够多分区副本复制的消息是“不安全”的——如果首领发生崩溃，另一个副本成为新首领，那么这些消息就丢失了。如果允许客户端读取只存在于首领中的消息，则可能会出现不一致的行为。试想，某个消费者先读取了一条消息，然后首领发生了崩溃，因为其他 broker 不包含这条消息，所以该消息就丢失了，其他消费者也就不可能读取到该消息，这样它们的行为与读取到该消息的 消费者就会不一致。所以，我们会等到所有同步副本都复制了消息，才允许消费者读取它们，如下图。这也意味着，如果 broker 间的消息复制因为某些原因变慢，那么消息到达消费者的时间也会变长(因为会先等待消息复制完毕)。最大延迟时间可以通过参数 `replica.lag.time.max.ms` 来配置， 它指定了分区副本在复制消息时最多出现多长的延迟仍然被认为是同步的。

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/kafka/kafka-14.png" alt="image" style={{ maxWidth: '80%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>

[//]: # (![image]&#40;/Users/liufei/docs/typora笔记/images/kafka/kafka-14.png&#41;)

#### broker处理返回消息

如果请求的偏移量、分区信息等正确，那么 broker 将按照客户端指定的数量上限从分区中读取消息，再把消息返回给客 户端。**Kafka使用零拷贝技术向客户端发送消息，也就是说，Kafka 会直接把消息从文件(或者更确切地说是 Linux 文件系统缓存)里发送到网络通道，不需要经过任何中间缓冲区。**这是 Kafka 与其他大部分数据库系统不一样的地方，其他数据库在将数据发送给客户端之前会先把它们保存在本地缓存中。这项技术避免了字节复制，也不需要管理内存缓冲区，从而能够获得更好的性能。



### 文件存储

Kafka 的基本存储单元是分区。分区既无法在多个 broker 间再细分，也无法在同一个 broker 的多个磁盘间再细分。所以，分区的大小受单个挂载点可用空间的限制(一个挂载点可以是单个磁盘或多个磁盘RAID）。



**Kafka存储分层**

Kafka的存储是分层的，在分层存储架构中，Kafka 集群配置了两个存储层:本地存储层和远程存储层。本地存储层和当前的 Kafka 存储层一样，使用 broker 的本地磁盘存储日志片段，远程存储层则使用 HDFS、S3 等专用存储系统存储日志片段。

Kafka 用户可以单独为每一层配置保留策略。由于本地存储的成本通常远高于远程存储，因此本地存储的数据保留时间通常是几小时，甚至更短，而远程存储的保留时间则比较长，可以是几天，甚至几个月。

本地存储的延迟明显低于远程存储。对延迟敏感的应用程序通常从本地存储的分区尾部读取数据，因此可 以受益于现有的 Kafka 存储机制，比如可以有效地利用页面缓存。在进行数据回填或故障恢复时，应用程 序需要用到旧数据，所以需要从远程存储读取。

>  分层存储架构让 Kafka 集群的存储扩展可以独立于内存和 CPU，**因此可以将 Kafka 作为一种长期的存储解决方案。这既减少了存储在 broker 上的数据量，也减少了在进行故障恢复和再均衡时需要复制的数据量。** 远程存储中的日志片段不需要恢复到 broker 上，当然，如果有必要也可以进行按需恢复。因为不是所有的数据都存储在 broker 上，所以要延长集群数据保留时间就不再需要扩展集群存储或添加新节点。与此同时，延长系统总体数据保留时间也无须像其他系统那样使用单独的数据管道将数据从 Kafka 复制到外部存储。



**批次**

Kafka生产者都是以批次的方式发送消息。如果每次只发送一条消息，那 么使用批次反而会增加开销。但如果每次发送两条或更多的消息，那么使用批次就可以节约空间，减少网 络带宽和磁盘的使用。这也是为什么配置了 linger.ms=10，Kafka 会表现得更好的一个原因——要求 的延迟越小，消息被放在同一个批次里发送的可能性就越高。因为 Kafka 会为每个分区创建一个单独的批 次，所以写入的分区越少，生产者的效率就越高。需要注意的是，生产者可以在同一个生产请求中包含多 个批次。如果生产者端使用了压缩(推荐这么做)，那么更大的批次将意味着不管是通过网络传输还是磁 盘保存都能获得更好的压缩比。

> Kafka 消息由有效负载和系统标头组成。有效负载包括一个可选的键、值和一些可选的用户标头，其中每 个标头也是一个键–值对。每条记录的开销其实很小，大多数系统信息是批次级别的。标头中只保存批次第一条消息的偏移量和时间戳，每条记录中只保存差值，这极大地减少了每条记录的开销，从而让传输更大的批次变得更加高效。



#### 数据目录结构

Kafka 的文件目录结构包含各种用于存储日志数据、索引文件、配置文件等的重要目录和文件。典型的 Kafka 文件目录结构如下：

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/kafka/kafka-15.png" alt="image" style={{ maxWidth: '56%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>

[//]: # (<img src="/Users/liufei/docs/typora笔记/images/kafka/kafka-15.png" alt="image" style="zoom:36%;" />)

- **运行目录**：

- `bin/`：包含 Kafka 的启动脚本和命令行工具，如 `kafka-server-start.sh`、`kafka-topics.sh` 等。
- `config/`：包含 Kafka 的配置文件，如 `server.properties`、`zookeeper.properties` 等。
- `libs/`：包含 Kafka 所需的所有依赖库文件。
- `logs/`：包含 Kafka 的运行时日志文件，记录了 Kafka 服务器的各种运行信息。

- **数据目录**（由 `log.dirs` 配置指定，通常为 `/var/lib/kafka` 或 `/tmp/kafka-logs`）：

- Topic 目录：每个 topic 有一个独立的目录，目录名为`topicName-partitionNumber`比如`my-topic-0`

- 日志段文件：以 `.log` 为扩展名，每个分区日志被分成多个日志段文件。每个文件包含了实际的消息数据，文件名为日志段的起始偏移量，例如 `00000000000000000000.log`。

- 索引文件：以 `.index` 和 `.timeindex` 为扩展名，索引文件加速了 Kafka 查找特定偏移量或时间的消息，文件名与对应的日志段文件一致，例如 `00000000000000000000.index` 和 `00000000000000000000.timeindex`。

>  `.index` ：为了帮助 broker 更快定位到指定的偏移量，Kafka 为每个分区维护了一个索引，其索引为稀疏索引（Sparse index file）,默认是日志写入大小达到4KB时，才会在.index中增加一个索引项。可以通过log.index.interval.bytes来设置这个间隔大小。该索引将偏移量与片段文件以及偏移量在文件中的位置做了映射。
    >
    >  `.timeindex` Kafka 还有第二个索引，该索引将时间戳与消息偏移量做了映射。在按时间戳搜索消息时会用到 这个索引。这种搜索方式在 Kafka Streams 中使用广泛，在一些故障转移场景中也很有用。
    >
    >  **注意：索引也会被分成片段，所以，在删除消息时也可以删除相应的索引。Kafka 没有为索引维护校验和。如果 索引损坏，那么 Kafka 将通过重新读取消息并记录偏移量和位置来再次生成索引。如果有必要，管理员也 可以删除索引，这样做绝对安全(尽管可能需要较长的恢复时间)，因为 Kafka 会自动重新生成索引。**

- 检查点文件：以 `.checkpoint` 为扩展名，保存了日志段文件的检查点信息，用于在重启时恢复日志状态。

- **管理目录**（如果使用 Zookeeper作为集群协调组件，通常为 `/var/lib/zookeeper` 或 `/tmp/zookeeper`）：

- `data/`：包含 Zookeeper 的数据文件。
- `version-2/`：Zookeeper 数据文件的默认目录。
- `log/`：包含 Zookeeper 的事务日志文件。



#### 文件删除策略

Kafka 不会一直保留数据，也不会一直等到消息被所有消费者读取了 之后才将其删除。相反，Kafka 管理员会为每个主题配置数据保留期限，主题的数据要么在达到指定的时 间之后被清除，要么在达到指定的数量之后被清除。

在一个大文件中查找和删除消息既费时又很容易出错，所以我们会把分区分成若干个 。在默认情况 下，每个片段包含 1 GB 或一周的数据，以较小的那个为准。在 broker 向分区写入数据时，如果触及任意 一个上限，就关闭当前文件，并打开一个新文件。

*当前正在写入数据的片段叫作 。活动片段永远不会被删除，所以，如果你配置的保留时间是 1 天，但片段里包含了 5 天的数据，那么这些数据就会被保留 5 天，因为在片段被关闭之前，这些数据是不会被删除的。如果你要保留数据一周，并且每天使用一个新片段，那么每天就会有一个新片段被创建，同时最旧的一个片段会被删除，因此这个分区在大部分时间里会有 7 个片段。*

> broker 会为分区的每一个打开的日志片段分配一个文件句柄，哪怕是非活动片段。这样就会 打开很多文件句柄，因此必须根据实际情况对操作系统做一些调优。



#### 日志压缩(compact log)

Kafka的消息就相当于日志的历史状态序列，每次状态发生变化就将新状态写入 Kafka。比如当应用程序从故障中恢复时它会从 Kafka 读取之前保存的消 息，以便恢复到最近的状态。应用程序只关心发生崩溃前的那个状态。以及业务场景中一系列的数据变更，最后实际上只关心最新的数据状态并不关心在运行过程中发生的所有状态变化。所以Kafka的消息常常有压缩的需求，以便快速读取最新的消息并且减少数据的存储压力。而因为Kafka消息是都是kv结构，实际上是相当有利于消息压缩的。

Kafka 可以设置时间来配置保留数据的策略，如果保留策略是 `delete`，那么早于保留时间的旧事件将被删除；如果保留策略是 `compact`，那么只为每个键保留最新的值。主题的数据保留策略也可以被设置成 `delete.and.compact`，也就是以上两种策略的组合。超过保留时间的消息将被删除，即使它们的键对应的值是最新的。组合策略可以防止压实主题变得太大，同时也可以 满足业务需要在一段时间后删除数据的要求。

> 注意：只有当应用程序生成的事件里包含了键–值对时，设置 compact 才有意义。如果主题中包含了 null 键，即发生消息的`ProducerRecord<K, V>`中，key为空，那么这个策略就会失效。因为和Kafka的压缩原理有关。



**日志压缩的工作原理**

Kafka 的日志压缩机制（log compaction）是一种用于持久化消息并维护每个键的最新值的策略。它通常用于需要持久化每个键的最新状态的场景，例如数据库变更日志、物联网传感器数据等。压缩的原理如下：

- **基于键的压缩**：Kafka 的日志压缩是基于消息键（key）进行的。Kafka 会定期对日志分区进行扫描和压缩，删除同一个键的旧版本消息，仅保留最新的一个。

- **消费后进行压缩**：日志压缩通常在消息被消费者消费之后进行。Kafka 不会在消息生产时立即进行压缩，而是等待一定时间后对日志进行压缩操作。

- **保留最新的值**：日志压缩会确保每个键至少有一个最新的值被保留。如果某个键已经被删除（通过发送带有 null 值的消息），日志压缩也会记录这种删除操作，确保消费者可以感知到删除行为。

具体过程：

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/kafka/kafka-16.png" alt="image" style={{ maxWidth: '50%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>

[//]: # (<img src="/Users/liufei/docs/typora笔记/images/kafka/kafka-16.png" alt="image" style="zoom:50%;" />)

"干净"部分：这些消息之前被压实过，每个键只有一个对应的值，这个值是上一次压实时保留下来的。

"浑浊"部分：这些消息是在上一次压实之后写入的。

开启日志压缩之后，broker 在启动时会创建一 个压实管理器线程和一些压缩工作线程来执行压缩任务。这些线程会选择浑浊率(浑浊的消息占分区总体 消息的比例)最高的分区来压实。压缩线程会读取分区的浑浊部分，并在内存中创建一个 map。map 的每个元素都包含消息 键的哈希值(16 字节)和上一条具有相同键的消息的偏移量(8 字节)。也就是说，每个 map 的元素只 占用 24 字节的内存。假设要压实一个 1 GB 的日志片段，每条消息大小为 1 KB，总共有 100 万条消息， 那么只需要用 24 MB 的 map 就可以压实这个片段。(如果有重复的键，则可以重用哈希项，从而使用更 少的内存。)这个效率是非常高的。

Kafka 管理员可以配置压实线程在执行压实时可以为 map 分配多少内存。每个线程都会创建自己的 map， 但这个参数指的是所有线程可使用的内存总大小。如果你为 map 分配了 1 GB 内存，并使用了 5 个压实线 程，那么每个线程将可以使用 200 MB 内存。Kafka 不要求这个 map 可以放下整个分区的浑浊部分，但至少要能够放下一个片段的浑浊部分，否则 Kafka 会报错。管理员要么为 map 分配更多的内存，要么减少压实线程数量。如果有几个片段都可以被放进 map，那么 Kafka 将从最旧的片段开始压实，其他片段则继 续保持浑浊，然后等待下一轮压缩。

压缩之后的结果如下：

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/kafka/kafka-17.png" alt="image" style={{ maxWidth: '36%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>

[//]: # (<img src="/Users/liufei/docs/typora笔记/images/kafka/Kafka-17.png" alt="image" style="zoom:36%;" />)





> 注意：日志压缩并不适用于所有场景，比如每条消息虽然key是相同的，但value是不同的，而且value是相互独立的，如key是会话id，value是会话中的文本消息，这样每条Kafka消息来都是新的对话消息。此时如果压缩日志就会导致消息丢失。不能开启日志压缩，配置 `log.cleaner.enabled `参数为false来关闭日志压缩。



#### 日志删除

**开启日志压缩时删除某条消息**

要彻底把一个键从系统中删除，应用程序必须发送包含这个键且值为 null 的消息，即如果开启压缩，则压缩线程在发现这条消息时，会先进行常规的压实操作，只保留值为 null 的消息。这条消息(被称为 )会根据配置 的参数保留一段时间。在此期间，消费者可以读取到这条消息，并且发现它的值已经被置空。消费者在将 Kafka 数据复制到关系数据库时，如果它看到这条墓碑消息，就知道应该要把相关的用户信息从数据库中 删除。在超过保留期限之后，清理线程会移除墓碑消息，它们的键也将从 Kafka 分区中消失。这里的关键 是要让消费者有足够的时间看到墓碑消息，因为如果消费者离线几小时，那么可能就错过了墓碑消息，也 就不会去删除数据库中的相关数据了。

**常规日志删除**

如果没有开启日志压缩（即使用默认的日志删除策略 `delete`），Kafka 不会基于键来删除消息。相反，它会按照设定的日志保留时间或日志段大小来删除旧的消息。因此，在这种情况下，发送包含键且值为 `null` 的消息并不会导致该键被删除，而是会像普通消息一样被保留直到达到日志保留时间或日志段大小限制。



> 值得一提的是，Kafka 的管理客户端提供了一个 deleteRecords 方法。这个方法可用于删除指定偏移量 之前的所有记录，但它使用的是一种完全不同的机制。当这个方法被调用时，Kafka 会将低水位标记(分区的第一个偏移量)移动到指定的偏移量。这样可以防止消费者读取低水位标记之前的记录，保证这些记录在被清理线程删除之前都是不可访问的。这个方法可用于删除设置了保留策略的主题和压实主题。



## Kafka系统关键特性

### 消息可靠性

消息可靠性即如何保证消息不丢失并且是正确的，而可靠性是系统而不是某个独立组件的一个属性，所以，在讨论 Kafka 的可靠性保证时，需要从系统的整体 出发。那些与 Kafka 集成的系统与 Kafka 本身一样重要。

**Kafka的消息可靠性保证**

Kafka提供如下的特性来保证消息的可靠性：

- Kafka 可以保证分区中的消息是有序的。如果使用同一个生产者向同一个分区中写入消息，并且消息 B 在消息 A 之后写入，那么 Kafka 可以保证消息 B 的偏移量比消息 A 的偏移量大，而且消费者会先 读取消息 A 再读取消息 B。
- 消息只有在被写入分区所有的同步副本时才被认为是“已提交”的(但不一定要冲刷到磁盘上)。 生产者可以选择接收不同类型的确认，比如确认消息被完全提交，或者确认消息被写入首领副本，或 者确认消息被发送到网络上。
- 只要还有一个副本是活动的，已提交的消息就不会丢失。
- 消费者只能读取已提交的消息。

并且在涉及Kafka的几个核心部分都需要正确的配置才能保证可靠性。

#### broker的可靠性配置

broker 中有 3 个配置参数：复制系数、分区选主和副本同步，会影响 Kafka 的消息存储可靠性。与其他配置参数一样，它们既可以配置在 broker 级别，用于控制所有主题的行为，也可以配置在主题级别，用于控制个别主题的行为。

**复制系数**

Kafka 默认的复制系数是3 ，即每个分区总共会有3个副本，不过可以修改，即使是在主题被创建之后，仍然可以使用 Kafka 的副本分配工具新增或移除副本，以此来改变复制系数。主题级别的配置参数是 `replication.factor`，在 broker 级别通过 `default.replication.factor` 来设置自动创建的主题的复制系数。

> 如果复制系数是 *N*，那么在 *N*–1 个 broker 失效的情况下，客户端仍然能够从主题读取数据或向主题写入数据。所以，更高的复制系数会带来更高的可用性、可靠性和更少的灾难性事故。另外，复制系数 *N* 需要至少 *N* 个 broker，也就是说我们会有 *N* 个数据副本，并且它们会占用 *N* 倍的磁盘空间。基本上就是在用硬件换取可用性。

如何设置副本数需要考虑如下因素：

- 可以性和持久性：如果一个分区只有一个副本，那么它在 broker 例行重启期间将不可用。副本越多，可用性就越高。每个副本都包含了一个分区的所有数据。如果一个分区只有一个副本，那么一旦磁盘损坏，这个分区
的所有数据就丢失了。如果有更多的副本，并且这些副本位于不同的存储设备中，那么丢失所有副本的概
率就降低了。

- 吞吐量：每增加一个副本都会增加 broker 内的复制流量。如果以 10 MBps 的速率向一个分区发送数据，并且 只有 1 个副本，那么不会增加任何的复制流量。如果有 2 个副本，则会增加 10 MBps 的复制流量，3 个副 本会增加 20 MBps 的复制流量，5 个副本会增加 40 MBps 的复制流量。在规划集群大小和容量时，需要把这个考虑在内。

- 延迟：每一条记录必须被复制到所有同步副本之后才能被消费者读取。从理论上讲，副本越多，出现滞后的可能性就越大，因此会降低消费者的读取速度。在实际当中，如果一个 broker 由于各种原因变慢，那么它就会影响所有的客户端，而不管复制系数是多少。

- 成本：数据副本越多存储和网络成本就越高。所以出于成本方面的考虑，非关键数据的复制系数一般小于3。



**分区选主**

当分区的首领不可用时，一个同步副本将被选举为新首领。如果在选举过程中未丢失数据，也就是说所有同步副本都包含了已提交的数据，那么这个选举就是正确的。但如果在首领不可用时其他副本都是不同步的，此时就要面临选择了，具体的场景如下：

> 分区有 3 个副本，其中的两个跟随者副本不可用(比如有两个 broker 发生崩溃)。这个时候，随着生产者继续向首领写入数据，所有消息都会得到确认并被提交(因为此时首领是唯一的同步副本)。现在，假设首领也不可用了(又一个 broker 发生崩溃)，这个时候，如果之前的一个跟随者重新启动， 那么它就会成为分区的唯一不同步副本。
>
> 分区有 3 个副本，由于网络问题导致两个跟随者副本复制消息滞后，因此即使它们还在复制，但已经不同步了。作为唯一的同步副本，首领会继续接收消息。这个时候，如果首领变为不可用，则只剩下两个不同步的副本可以成为新首领。

这个时候需要作出选择，就是：

> 如果不允许不同步的副本被提升为新首领：那么分区在旧首领(最后一个同步副本)恢复之前是不可 用的。有时候这种状态会持续数小时(比如更换内存芯片)。
>
> 如果允许不同步的副本被提升为新首领：那么在这个副本变为不同步之后写入旧首领的数据将全部丢 失，消费者读取的数据将会出现不一致。为什么会这样?假设在副本 0 和副本 1 不可用的情况下，向副本 2(也就是首领)写入偏移量为 100-200 的消息。现在，副本 2 变为不可用，副本 0 变为可用， 但副本 0 只包含偏移量为 0~100 的消息。如果允许副本 0 成为新首领，那么生产者就可以继续写入数据，消费者则可以继续读取数据。于是，新首领就有了新的偏移量为 100-200 的新消息。这样，部分消费者会读取到偏移量为 100-200 的旧消息，部分消费者会读取到偏移量为 100-200 的新消息，还有部分消费者会读取到二者的混合消息。这就导致了数据的不一致。另外，副本 2 可能会重新变为可用，并成为新首领的跟随者。这个时候，它会把在当前首领中不存在的消息全部删除，以致所有消费者都将无法读取到这些消息。**--注意不管是zookeeper适用的zab算法还是kraft适用的raft算法，都是主节点向追随节点同步消息而不存在追随节点向主节点同步消息，所以如果不同不的追随节点成为主节点，之前主节点未同步到此新晋的主节点的数据就会全部丢失。主节点会想追随节点发生自己的同步请求来覆盖追随者的数据，这个就是分布式算法中的副本数据合并。**



**这就需要做选择，即就是分布式中的AP和CP的选择，即要么选AP要可用性但承担丢失数据和消费者读取到不一致的数据的风险，要么选CP选一致性但接受较低的可用性，因为必须等待原先的首领恢复到可用状态。**

通过参数：`unclean.leader.election.enable`来设置(这个参数只能设置broker不能设置topic)。在默认情况下的值是 false，也就是不允许不同步副本成为 首领。这是最安全的选项，因为它可以保证数据不丢失。这也意味着在之前描述的极端不可用场景中，一些分区将一直不可用，直到手动恢复。当遇到这种情况时，管理员可以决定是否允许数据丢失，以便让分区可用，如果可以，就在启动集群之前将其设置为true，在集群恢复之后不要忘了再将其改回 false。



**副本同步**

配置当分区中的副本有多少同步的情况下，消息才可用，配置参数：`min.insync.replicas` 。

> 假如一个主题配置了 3 个副本，出现只剩下一个同步副本的情况。如果这个同步副本变为不可用，则必须在可用性和一致性之间做出选择，而这是一个两难的选择。根据 Kafka 对可靠性保证 的定义，一条消息只有在被写入所有同步副本之后才被认为是已提交的，但如果这里的“所有”只包含一个同步副本，那么当这个副本变为不可用时，数据就有可能丢失。

如果想确保已提交的数据被写入不止一个副本，就要把最少同步副本设置得大一些。对于一个包含 3 个副 本的主题，如果 min.insync.replicas 被设置为 2，那么至少需要有两个同步副本才能向分区写入数据。

如果 3 个副本都是同步的，那么一切正常进行。即使其中一个副本变为不可用，也不会有什么问题。但是，如果有两个副本变为不可用，那么broker 就会停止接受生产者的请求。尝试发送数据的生产者会收到 `NotEnoughReplicasException` 异常，不过消费者仍然可以继续读取已有的数据。实际上，如果使用这样的配置，那么当只剩下一个同步副本时，它就变成只读的了。这样做是为了避免在发生不彻底的选举 时数据的写入和读取出现非预期的行为。要脱离这种只读状态，必须让两个不可用分区中的一个重新变为 可用(比如重启 broker)，并等待它变为同步的。

同时要确保副本同步的正确配置来保证数据的一致性，还有一些参数需要注意：

- `zookeeper.session.timeout.ms` 是允许 broker 不向 ZooKeeper 发送心跳的时间间隔。如果超过这个时间不发送心跳，则 ZooKeeper 会认为 broker 已经“死亡”，并将其从集群中移除。

- `replica.lag.time.max.ms` 指定的时间内从首领复制数据或赶上首领，那么它将变成不同步副本。*这个参数也会影响Kafka的可用性，如果写入的消息需要同步所有节点确认才能被读取，这个参数就可能是的单个很长的响应，影响整体的可用性。*


#### 生产者的可靠性配置

要保证生产者发生消息时，消息可靠发送，需要处理下面两个场景：

- 配置恰当的 acks。
- 正确配置重试参数，并在代码里正确处理异常。



**acks配置**

- acks=0：如果生产者能够通过网络把消息发送出去，那么就认为消息已成功写入 Kafka。不过，在这种情况下 仍然有可能出现错误，比如发送的消息对象无法被序列化或者网卡发生故障。如果此时分区离线、正在进 行首领选举或整个集群长时间不可用，则并不会收到任何错误。

> 在 acks=0 模式下，生产延迟是很低的，但它对端到端延迟即副本之间同步并不会带来任何改进(在消息被所有可用副本复制之前，消费者是看不到它们的)。
  >
  > 端到端：指从生产者（producer）发送消息到消费者（consumer）接收消息的整个数据流动过程。这个过程包括多个步骤和机制，确保数据的可靠传递、持久性和一致性。

- acks=1：首领在收到消息并把它写入分区数据文件(不一定要冲刷到磁盘上)时会返回确认或错误响应。而如果首领被关闭或发生崩溃，那么那些已经成功写入并确认但还没有被跟随者复制的消息就丢失了。另外，消息写入首领的速度可能比副本从首领那里复制消息的速度更快，这样会导致分区复制不及时，因为首领在消息被副本复制之前就向生产者发送了确认响应。

- acks=all：首领在返回确认或错误响应之前，会等待所有同步副本都收到消息。这个配置可以和 `min.insync.replicas` 参数结合起来，用于控制在返回确认响应前至少要有多少个副本收到消息。**这是最安全的选项，因为生产者会一直重试，直到消息提交成功。不过，这种模式下的生产者延迟也最大， 因为生产者在发送下一批次消息之前需要等待所有副本都收到当前批次的消息。**



**重试参数**

生产者需要处理的错误包括两个部分:一部分是由生产者自动处理的错误，另一部分是需要开发者手动处
理的错误。

当生产者向 broker 发送消息而broker 可以返回错误响应。错误响应可以分为两种，一种是在重试之后可以解决的，另一种是无法通过重试解决的。如果 broker 返回 `LEADER_NOT_AVAILABLE` 错误，那么生产者可以尝试重新发送消息——或许新首领被选举 出来了，那么第二次尝试发送就会成功。也就是说，`LEADER_NOT_AVAILABLE` 是一个 错误。如 果 broker 返回 `INVALID_CONFIG` 错误，那么即使重试发送消息也无法解决这个问题，所以这样的重试是没有意义的，这是 错误。

**但如果你的目标是不丢失消息，那么就让生产者在遇到可重试错误时保持重试，最好的重试方式是使用默认的重试次数(整型最大值或无限) **，并把 `delivery.timeout.ms` 配置成我们愿意等待的时长，生产者会在这个时间间隔内一直尝试发送消息。

> 重试参数`retries`，`properties.put(ProducerConfig.RETRIES_CONFIG, 3);`

重试发送消息存在一定的风险，因为如果两条消息都成功写入，则会导致消息重复。通过重试和小心翼翼 地处理异常，可以保证每一条消息都会被保存 ，但不能保证 。如果把 enable.idempotence 参数设置为 true，那么生产者就会在消息里加入一些额外的信息，broker 可以 使用这些信息来跳过因重试导致的重复消息。



**异常处理**

使用生产者内置的重试机制可以在不造成消息丢失的情况下轻松地处理大部分错误，但开发人员仍然需要
处理以下这些其他类型的错误。

- 不可重试的 broker 错误，比如消息大小错误、身份验证错误等。
- 在将消息发送给 broker 之前发生的错误，比如序列化错误。
- 在生产者达到重试次数上限或重试消息占用的内存达到上限时发生的错误。
- 超时。



#### 消费者的可靠性配置

保证消费者行为的可靠性很关键的就是**需要正确处理提交偏移量offset，并且需要注意的是提交的offset是分区单调递增的长整型，而且含义是之前的比offset小的所有消息都已经消费，而不是当前的消息被消费**，并且需要注意以下 4 个非常重要的配置参数：

1. group.id：如果两个消费者具有相同的群组 ID，并订阅了同一个主题，那么每个消费者将分到主题分区的一个子集，也就是说它们只能读取到所有消息的一个子集(但整个群组可以读取到主题所有的消息)。如果你希望一个消费者可以读取主题所有的消息，那 么就需要为它设置唯一的 group.id。

2. auto.offset.reset：指定当没有偏移量(比如在消费者首次启动时)或请求的偏移量在 broker 上不存在时消费者该作何处理。两个值：

- earliest：消费者从分区的开始位置读取数据，即使它没有有效的偏移 量。这会导致消费者读取大量的重复数据，但可以保证最少的数据丢失。
- latest：消费者从分区的末尾位置读取数据。这样可以减少重复处理消息，但很有可能会错过一些消息。

3. enable.auto.commit：消费者自动提交偏移量，也可以在代码里手动提交偏移 量。自动提交的一个最大好处是可以少操心一些事情。如果是在消费者的消息轮询里处理数据，那么自动提交可以确保不会意外提交未处理的偏移量。

> 自动提交的主要缺点是我们无法控制应用程序可能重复处理 的消息的数量，比如消费者在还没有触发自动提交之前处理了一些消息，然后被关闭。
   >
   > 如果应用程序的处 理逻辑比较复杂(比如把消息交给另外一个后台线程去处理)，那么就只能使用手动提交了，因为自动提 交机制有可能会在还没有处理完消息时就提交偏移量。

4. auto.commit.interval.ms：与第三个参数有直接的联系。如果选择使用自动提交， 那么可以通过这个参数来控制提交的频率，默认每 5 秒提交一次。因为频繁提交会增加额外的开销，但也会降低重复处理消息的概率。如果一个消费者经常因为发生再均衡而暂停处理消息，则很难说它是可靠的，尽管这与数据处理的可靠性没有直接关系。



### 精确一次性语义

精确一次性语义即消息不重复。Kafka 的精确一次性语义由两个关键特性组成:

- 幂等生产者(避免因重试导致的消息重复)
- 事务语义 (保证流式处理应用程序中的精确一次性处理)。



#### 幂等生产者

Kafka启用了幂等生产者`props.put("enable.idempotence", "true")`，那么每条消息都将包含生产者 ID(PID)和序列号。将它们与目标主题和分区组合在一起，用于唯一标识一条消息。broker 会用这些唯一标识符跟踪写入每个分区的最后n条消息，默认是5条。

> 为了减少每个分区需要跟踪的序列号数量，生产者需要将 `max.inflight.requests` 设置成 5 或更小的值(默认值是 5)。

如果 broker 收到之前已经收到过的消息，那么它将拒绝这条消息，并返回错误。生产者会记录这个错误， 并反映在指标当中，但不抛出异常，也不触发告警。在生产者客户端，错误将被添加到 `record-error- rate` 指标当中。在 broker 端，错误是 `ErrorsPerSec` 指标的一部分(`RequestMetrics` 类型)。

**生产者幂等实现**

为了获取生产者 ID，生产者在启动时会调用一个额外的 API。 每个消息批次里的第一条消息都将包含生产者 ID 和序列号(批次里其他消息的序列号基于第一条消 息的序列号递增)。这些新字段给每个消息批次增加了 96 位(生产者 ID 是长整型，序列号是整 型)，这对大多数工作负载来说几乎算不上是额外的开销。broker 将会验证来自每一个生产者实例的序列号，并保证没有重复消息。并且每个分区的消息顺序都将得到保证，即使 `max.in.flight.requests.per.connection` 被设置 为大于 1 的值(5 是默认值，这也是幂等生产者可以支持的最大值)。

**序列号异常**

如果 broker 收到一个非常大的序列号该怎么办? 比如broker 期望消息 2 后面跟着消息 3，但收到了消息 27，那么这个时候该怎么办?在这种情况下，broker 将返回“乱序”错误。如果使用了不带事务的幂等生产者，则这个错误可能会被忽略。

*虽然生产者在遇到“乱序”异常后将继续正常运行，但这个错误通常说明生产者和 broker 之间 出现了消息丢失——如果 broker 在收到消息 2 之后直接收到消息 27，那么说明从消息 3 到消息 26 一 定发生了什么。如果你在日志中看到这样的错误，那么最好重新检查一下生产者和主题的配置，确保 为生产者配置了高可靠性参数，并检查是否发生了不彻底的首领选举。*

*而且生产者重启，broker将无法检测到重复，因为会有不同的生产者id，并且因为生产者的信息是维护在主分区上的，如果broker故障重新选取主分片，也可能导致元信息丢失也是可能无法检测生产者消息重复*



**使用幂等生产者是在进行重试时避免消息重复的最简单的方法，通常建议使用生产者内置的重试机制，而不是在应用程序中捕获异常并自行进行重试。**

**但幂等生产者只能防止由生产者内部重试逻辑引起的消息重复。对于使用同一条消息调用两次 producer.send() 就会导致消息重复的情况，即使使用幂等生产者也无法避免。**



#### 事务

Kafka 的事务机制是专门为流式处理应用程序而添加的。因此，它非常适用于流式处理应用程序的基础模式，即“消费–处理–生产”。事务可以保证流式处理的精确一次性语义——在更新完应用程序内部状态并将结果成功写入输出主题之后，对每个输入消息的处理就算完成了。

所以消费事务也是一样的处理方式，因为消费时提交offset其实就是向内置的`_consumer_offsets`主题写入消息。这样精确一次处理意味着消费、处理和生产都是原子操作，要么提交偏移量和生成结果这两个操作都成功，要么都不成功。我们要确保不会出现只有部分操作执行成功的情况(提交了偏移量但没有生成结果，反之亦然)。



##### 事务精准一次性的实现

为了支持流式事务，Kafka引入了原子多分区写入的概念。提交偏移量和生成结果都涉及向分区写入数据，结果会被写入输出主题，偏移量会被写入 `_consumer_offsets` 主题。如果可以打开一个事务，向这两个主题写入消息，如果两个写入操作都成功就提交事务，如果不成功就中止，并进行重试，那么就会实现我们所追求的精确一次性语义。

如下图：执行原子多分区写入的同时提交消息偏移量

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/kafka/kafka-18.png" alt="image" style={{ maxWidth: '56%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>

[//]: # (<img src="/Users/liufei/docs/typora笔记/images/kafka/kafka-18.png" alt="image" style="zoom:56%;" />)



##### 生产者端的事务

注意事务中，生产者必须有如下的配置：

1. **`enable.idempotence`**：必须设置为 `true`，因为事务性生产者依赖于幂等性特性来保证消息的唯一性。

2. **`transactional.id`**：必须配置一个唯一的事务 ID 来标识生产者的事务性操作。这个 ID 对每个生产者实例必须是唯一的，并且在重启时保持一致。
3. **`acks`**：必须设置为 `all`，以确保消息被所有副本确认，从而提供更高的可靠性。

因为启用事务和执行原子多分区写入，事务性生产者实际上就是一个配置了`transactional.id`并用`initTransactions()`方法初始化的 Kafka 生产者。与`producer.id`(由 broker 自动生成)不同，`transactional.id`是一个生产者配置参数，在生产者重启之后仍然存在。实际上，`transactional.id`主要用于在重启之后识别同一个生产者。broker 维护了`transactional.id`和`producer.id`之间的映射关系，如果对一个已有的`transactional.id`再次调用`initTransactions()` 方法，则生产者将分配到与之前一样的`producer.id`，而不是一个新的随机数。

同时Kafka 会增加与 transactional.id 相关的 epoch，类似于`raft`算法中领导者任期。带有相同 transactional.id 但 epoch 较小的发送请求、提交请求和中止请求将被拒绝，并返回`FencedProducer`错误。旧生产者将无法写入输出流， 并被强制 close()，以防止“僵尸”(“僵尸”的含义就是一个消费者即读取了消息但没有处理而宕机，此时其分区被重新给别的消费者组的的消费者，而后续又恢复回来继续处理之前的消息)引入重复记录。



##### 消费者端的事务

创建事务性生产者、开始事务、将记录写入多个分区、生成偏移量并提交或中止事务，这些都是由生产者完成的。然而，这些还不够。以事务方式写入的记录，即使是最终被中止的部分，也会像其他记录一样被写入分区。消费者也需要配置正确的隔离级别，否则将无法获得想要的精确一次性保证。

通过设置`isolation.level`参数来控制消费者如何读取以事务方式写入的消息。如果设置为`read_committed`，那么调用`consumer.poll()`将返回属于已成功提交的事务或以非事务方式写入的消息，它不会返回属于已中止或执行中的事务的消息。默认的隔离级别是`read_uncommitted`，它将返回所有记录，包括属于执行中或已中止的事务的记录。配置成`read_committed`并不能保证应用程序可以读取到特定事务的所有消息。也可以只订阅属于某个事务的部分主题，这样就可以只读取部分消息。

> 应用程序无法知道事务何时开始或结束，或者哪些消息是哪个事务的一部分。

如图，不同的隔离级别读取的消息进度是不一样的：

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/kafka/kafka-19.png" alt="image" style={{ maxWidth: '50%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>

[//]: # (<img src="/Users/liufei/docs/typora笔记/images/kafka/kafka-19.png" alt="image" style="zoom:50%;" />)

为了保证按顺序读取消息，`read_committed`隔离级别将不返回在事务开始之后(这个位置也被叫作最后稳定偏移量，`last stable offset`-`LSO`)生成的消息。这些消息将被保留，直到事务被生产者提交或终 止，或者事务超时(通过`transaction.timeout.ms`参数指定，默认为 15 分钟)并被 broker 终止。

> **长时间使事务处于打开状态会导致消费者延迟，从而导致更高的端到端延迟。**

这样流式处理应用程序的输出结果具备了精确一次性保证，即使输入消息是以非事务方式写入的。原子多分区写入可以保证在将输出记录提交到输出主题的同时也提交了输入记录的偏移量，所以输入记录不会被重复处理。



##### Kafka流式事务示例和细节

```java
public static void main(String[] args) {
        Long transactionalId = 1L;

        String bootstrapServers = "127.0.0.1:9092,127.0.0.1:9093,127.0.0.1:9094";

        Properties producerProps = new Properties();
        producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, "DemoProducer");
        // 为生产者配置 transactional.id，让它成为一个能够进行原子多分区写入的事务性生产者。事务 ID 必须是唯一且长期存在的，因为本质上就是用它定义了应用程序的一个实例
        producerProps.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, transactionalId);
        KafkaProducer<String, String> producer = new KafkaProducer<>(producerProps);


        Properties properties = new Properties();
        String groupId = "my-group"; // 消费者组 ID
        String topic = "liufei-test"; // 目标主题
        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        properties.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "latest");
        // 消费者不提交自己的偏移量——生产者会将偏移量提交作为事务的一部分，所以需要禁用自动提交
        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");
        properties.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "read_committed");
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(properties);

        //事务性生产者要做的第一件事是初始化，包括注册事务 ID 和增加 epoch 的值
        producer.initTransactions();
        consumer.subscribe(Collections.singletonList(topic));

        while (true) {
            try {
                ConsumerRecords<String, String> records =
                        consumer.poll(Duration.ofMillis(200));
                if (records.count() > 0) {
                    // 开启流式事务
                    producer.beginTransaction();
                    for (ConsumerRecord<String, String> record : records) {
                        ProducerRecord<String, String> customizedRecord = transform(record);
                        producer.send(customizedRecord);
                    }
                    Map<TopicPartition, OffsetAndMetadata> offsets = consumerOffsets();
                    // 在这里提交消费者位移
                    producer.sendOffsetsToTransaction(offsets, consumer.groupMetadata());
                }
                producer.commitTransaction();
            } catch (ProducerFencedException | InvalidProducerEpochException e) {
                /*
                 * ProducerFencedException：当已经有相同 transactional.id 的生产者启动，再启动就会报错
                 * InvalidProducerEpochException：这个就是当相同 transactional.id生产者启动但其实已经有更新epoch的生产者在发送消息时，就会报此错
                 * 如果遇到这个异常，则说明应用程序实例变成“僵尸”了。应用程序实例可能由于某种原因被挂起 或断开连接，而另一个具有相同事务 ID 的应用程序实例已经在运行当中。
                 * 很有可能启动的事务已经被中止，其他应用程序正在处理这些记录。
                 * 这个应用程序实例除了优雅地“死去”，别无他法，此应用直接异常退出
                 */
                throw new KafkaException(String.format(
                        "The transactional.id %s is used by another process", transactionalId));
            } catch (KafkaException e) {
                /*
                 * 如果在提交事务时遇到错误，则可以中止事务，重置消费者偏移量位置，并进行重试。
                 * 重试之后，消费者仍然是从当前的位移开始读取，然后重新执行
                 * 这里没有超时或者重试次数限制，会一直重试
                 */
                producer.abortTransaction();
                resetToLastCommittedPositions(consumer);
            }
        }
    }
```

Kafka事务实现是靠内置的一个叫事务协调器的组件来控制事务的执行，具体的工作细节结合上面的代码，执行流程如下：

1. **事务开始**：生产者向事务协调器发送 `InitProducerId` 请求，事务协调器在 `__transaction_state` 主题中记录事务状态并返回给生产者一个唯一的 `ProducerId` 和 `Epoch`。

2. **记录消息**：生产者在发送消息时附带 `ProducerId` 和 `Epoch`。Kafka 代理在接收到消息时，将其写入对应的分区日志中，同时记录消息的事务状态为“未提交”。

3. **提交事务**：`commitTransaction()`生产者向事务协调器发送 `EndTxn` 请求，事务协调器将事务状态更新为“提交”并写入 `__transaction_state` 主题，通知所有涉及的分区代理提交事务，每个分区代理将事务消息的状态从“未提交”更新为“已提交”，使其对消费者可见。

4. **中止事务**：`abortTransaction()`生产者向事务协调器发送 `EndTxn` 请求，事务协调器将事务状态更新为“中止”并写入 `__transaction_state` 主题，通知所有涉及的分区代理中止事务，每个分区代理将事务消息的状态从“未提交”更新为“无效”，使其对消费者不可见。



##### Kafka事务的局限

> 注意：在 Kafka 中加入事务是为了提供多分区原子写入(不是读取)，并隔离流式处理应用程序中 的“僵尸”生产者。在“消费–处理–生产”流式处理任务中，事务为提供了精确一次性保证。在其他场景中，事务要么不起作用，要么需要做额外的工作才能获得想要的结果。



##### 事务对性能的影响

事务给生产者带来了一些额外的开销。事务 ID 注册在生产者生命周期中只会发生一次。分区事务注册最多会在每个分区加入每个事务时发生一次，然后每个事务会发送一个提交请求，并向每个分区写入一个额外的提交标记。事务初始化和事务提交请求都是同步的，在它们成功、失败或超时之前不会发送其他数据，这进一步增加了开销。

> 需要注意的是，生产者在事务方面的开销与事务包含的消息数量无关。因此，一个事务包含的消息越多，
> 相对开销就越小，同步调用次数也就越少，从而提高了总体吞吐量。

在消费者方面，读取提交标记会增加一些开销。事务对消费者的性能影响主要是在 read_committed 隔离级别下的消费者无法读取未提交事务所包含的记录。提交事务的时间间隔越长，消费者在读取到消息之 前需要等待的时间就越长，端到端延迟也就越高。

但是，消费者不需要缓冲未提交事务所包含的消息，因为 broker 不会将它们返回给消费者。由于消费者在读取事务时不需要做额外的工作，因此吞吐量不受影响。









