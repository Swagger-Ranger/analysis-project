---
title: Lucene手册1-索引
sidebar_position: 1
toc_min_heading_level: 2
toc_max_heading_level: 5
---

## Lucene 是什么

Lucene 是一个高性能、全功能的文本搜索引擎库，提供了高效的索引和搜索功能，而 Elasticsearch 在此基础上构建了一个分布式搜索和分析引擎。

Lucene允许你向自已的应用程序中添加搜索功能。Lucene能够把你从文本中解析出来的数据进行索引和搜索。Lucene并不关心数据来源、格式，甚至不关心数据的语种，只要能把它转换为文本格式即可。也就是说你可以索引和搜索存储在文件中的如下数据：远程Web服务器上的网页、本地文件系统中的文档、简单的文本文件、Word文档、XML文档、HTML文档或者PDF文档，或者其他能够从中提取文本信息的数据格式。

同样，你也可以利用Lucene来索引存储在数据库中的数据，以给你的用户提供一些其他数据库所不具备的诸如全文搜索等功能。一且你的应用程序集成了Lucene，用户就可以进行诸如`+George +Rice -eat-pudding、Apple-pie+Tiger、animal:monkey AND
food：banana` 等有着复杂查询条件的搜索。有了Lucene，你可以为电子邮件信息、归档邮件列表、即时聊天信息以及维基（Wiki）页面等信息进行索引和搜索。


<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/lucene/lucene-1.png" alt="image" style={{ maxWidth: '80%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>

[//]: # (<img src="/Users/liufei/docs/typora笔记/images/lucene/lucene-1.png" alt="img" style="zoom:80%;" />)





### Lucene的处理过程

获取内容 --> 建立文档 --> 分析文档 --> 索引文档

- 建立文档：将文本内容，转化为文档`document`和域`field`

可以使用语义分析器：从文本中提取信息，比如：位置、地点、日期、姓名等，来建立field

- 分析文档：将field中的文本，进行分词，然后词干提取、同义词关联、语法修正

可以使用snowball：词干提取器，等分析工具

- 索引文档 ：对文档建立索引

搜索处理过程就是从索引中查找单词，从而找到包含该单词的文档。搜索质量主要由查准率（Precision）和查全率（Recall）来衡量。查全率用来衡量搜索系统查找相关文档的能力；而查准率用来衡量搜索系统过滤非相关文档的能力。

> Lucene的benchmark/contrib模块来衡量你的搜索程序的查准率和查全率。



### lucene的一些关键类

lucene有几个核心类，在不同的版本中可能实现有区别，但要做的事情都是一致的，命名也是一致的，可以直接搜索对应类名就能获取对应核心类在当前版本代码的类。

#### index

- Indexwriter
- Directory
- Analyzer
- Document
- Field

**Indexwriter**：IndexWriter（写索引）是索引过程的核心组件。这个类负责创建新索引或者打开已有索引，以及向索引中添加、删除或更新被索引文档的信息。可以把IndexWriter看做这样一个对象：它为你提供针对索引文件的写入操作，但不能用于读取或搜索索引Indexwriter需要开辟一定空间来存储索引，该功能可以由Directory完成。

**Directory**：Directory类描述了Lucene索引的存放位置。它是一个抽象类，它的子类负责具体指定索引的存储路径。IndexWriter类构造方法就要依赖Directory类来指定索引存放位置。

**Analyzer**：文本文件在被索引之前，需要经过Analyzer（分析器）处理。Analyzer是由IndexWriter的构造方法来指定的，它负责从被索引文本文件中提取语汇单元，并提出剩下的无用信息。如果被索引内容不是纯文本文件，那就就需要先将其转换为文本文档（*比如：如果是语音就需要依赖asr来转化成文本*）。Analyzer是一个抽象类，而Lucene提供了几个类实现它。这些类有的用于跳过停用词（stopwords）（指一些常用的且不能帮助区分文档的词，如a、an、the、in和on等）；有的用于把语汇单元转换成小写形式，以使搜索过程能忽略大小写差别；除此之外，还有一些其他类。Analyzer是Lucene很重要的一部分，它的用途远远不止过滤输入这一项。**对于要将Lucene集成到应用程序的开发人员来说，选择什么样Analyzer的是程序设计中非常关键的一步**。

**Document**：Document（文档）是一些域（Field）的集合，Document实现结构比较简单，为一个包含多个Field对象的容器，而Field是指包含能被索引的文本内容的类。如果类比sql，Document可以理解为数据行、Field则是行中的某一列。Lucene只处理以Field实例形式出现的文本。即具体来说，Lucene的内核本身只处理`java.lang.String`、`java.io.Reader`对象和本地数字类型（如int和foat类型），而field就是其索引的点。

> `java.io.Reader` 是 Java 标准库中用于读取字符流的抽象类，它是字符输入流的超类，专门用于读取字符数据（而不是字节数据）

虽然各种类型的文档都能被索引和搜索，但处理非文本和非数字类型的文档过程并没有处理后两类文档简单直接。Indexer专注于针对文本文件的索引操作。因此为每一个检索到的文件创建一个Document实例，并向实例中添加各个域，然后将Document对象添加到索引中，这样就完成了文档的索引操作。

**Field**：索引中的每个文档都包含一个或多个不同命名的域，这些域包含在Field类中。每个域都有一个域名和对应的域值来精确控制Lucene索引操作各个域值。文档可能拥有不止一个同名的域。在这种情况下，域的值就按照索引操作顺序添加进去。在搜索时，所有域的文本就好像连接在一起，作为一个文本域来处理。



##### lucen和es的概念对应

**Lucene Directory**：是 Lucene 中用于存储索引数据的低级存储抽象，实际上对应一个文件系统目录，用于存储单个 Lucene 索引。是一个抽象类，主要有`FSDirectory`和`RAMDirectory`等，负责管理索引文件的读写，每个 Lucene 索引都对应一个 Directory，Directory 包含了索引的所有文件，如 `.fdt`、`.fdx`、`.tii`、`.tis` 等。

**Elasticsearch Shard**：shard（分片）是一个独立的 Lucene 索引实例，每个分片存储在一个 Lucene Directory 中。

**Elasticsearch Index**：逻辑上的数据集合，由多个分片即多个 Lucene 索引（即多个 Lucene Directory）组成。每个索引由多个主分片和副本分片组成，主分片和副本分片相互备份，确保数据高可用性和读取性能，并且es 自动管理分片的分配和再平衡。

> 简而言之：
>
> **Elasticsearch Index** = 多个 **Shard** 的逻辑集合
>
> **Elasticsearch Shard** = 一个 Lucene **Directory**



#### search

- IndexSearcher
- Term
- Query
- TermQuery
- TopDocs

**IndexSearcher**：IndexSearcher类用于搜索由IndexWriter类创建的索引：这个类公开了几个搜素方法，它是连接索引的中心环节。可以将IndexSearcher Z类看做一个以只读方式打开索引的类。它利用Directory实例来掌控前期创建的索引，然后才能提供大量的搜索方法，其中一些方法在它的抽象父类`Searcher`中实现；最简单的搜索方法是将单个Query对象和inttopN计数作为该方法的参数，并返回一个TopDocs对象。

**Term**:Term对象是搜索功能的基本单元。与Field对象类似，Term对象包含一对字符串元素：域名和单词（或域文本值）。注意Term对象还与索引操作有关。然而，由于Term对象是由Lucene内部创建的

```java
/**
* 寻找contents域中包含单词lucene的前10个文档，并按照降序排列这10个文档。由于TermQuery对象是从抽象父类Query派生而来，你可以  * 在声明左侧使用Query类型
*/
Query g s new TermQuery (new Term(" contents "，“lucene"））
TopDocs hits = searcher.search(a, 10）；
```

**Query**：Lucene含有许多具体的Query（查询）子类。Lucene基本的Query子类：TermQuery、BooleanQuery、PhraseQuery、PrefixQuery、PhrasePrefixQuery、TermRangeQuery、 Numeri cRangeQuery、FilteredQuery和SpanQuery。Query
是它们共同的抽象父类。它包含了一些非常私用的方法，其中最有趣的当属`setBoost（float）`方法，该方法能使你告知Lucene某个子查询相对其他子查询来说必须对最后的评分有更强贡献。

> `setBoost(float)` 方法用于设置文档或字段的提升因子（boost factor），从而影响该文档或字段在搜索结果中的相关性评分。提升因子是一个浮点数，用于增加或减少文档或字段的重要性。较高的提升因子会使得文档或字段在搜索结果中的排名更靠前。

**TermQuery**：TermQuery是Lucene提供的最基本的查询类型，也是简单查询类型之一。它用来匹配指定域中包含特定项的文档，如果类比sql就是field中分词后是否包含 *字符完全匹配* ，如果不分词field为`keyword` 则field的内容就是完全匹配term中的查询文本。

**TopDocs**：类是一个简单的指针容器，指针一般指向前N个排名的搜索结果，搜索结果即匹配查询条件的文档。TopDocs会记录前N个结果中每个结果的intdocID（可以用它来恢复文档）和浮点型分数。



## 构建索引

要实现高效搜索的前提就是对要搜索的数据构建合适的索引。

### 对搜索内容建模

文档document是Lucene索引和搜索的原子单位。文档为包含一个或多个域`field`的容器，而**域feild则依次包含“真正的”被搜索内容**。每个域都有一个标识名称，该名称为一个文本值或二进制值。当你将文档加人到索引中时，可以通过一系列选项来控制Lucene的行为。在对原始数据进行索引操作时，你得首先将数据转换成Lucene所能识别的文档和域。在随后的搜索过程中，被搜索对象则为域值；例如，用户在输入搜索内容title:lucene时，搜索结果则为标题域值中包含单词lucene的所有文档。

与数据库不同的是，Lucene没有一个确定的全局模式。也就是说，加入索引的每个文档都是独立的，它与此前加人的文档完全没有关系：它可以包含任意的域，以及任意的索引、存储和项向量操作选项。它也不必包含与其他文档相同的域。它甚至可以做到与其他文档内容相同，仅是相关操作选项有所区别Lucene的这种特性非常实用：这使你能够递归访问文档并建立对应的索引。**你可以随时对文档进行索引，而不必提前设计文档的数据结构表**。如果随后你想向文档中添加域，那么可以完成添加后重新索引该文档或重建索引即可。
Lucene 的灵活架构还意味着单一的索引可以包含表示不同实体的多个文档。例如，用一个文档的诸如名称和价格等域来表示零售产品，而用另一个文档的诸如姓名、年龄和性别等域来表示人，另外还可以使用一个不可达的“中间态”文档，该文档只包含有关索引或搜索程序的一些中间数据（比如最近一次更新索引的时间，或者被索引的产品目录），同时该“中间态”文档内容不在搜索结果中出现。Lucene和数据库之间第二个主要的区别是，Lucene要求你在进行索引操作时简单化或反向规格化原始数据。



#### 反范式化（Denormalization）

在数据库设计中，规范化（Normalization）是指将数据结构划分为多个表，减少数据冗余并确保数据一致性。然而，这种设计在某些场景下会导致查询性能问题，因为需要进行多个表的联接操作。

此处说的Denormalization和数据库设计中的"反范式"是一个意思。去规范化的核心思想是将相关数据直接嵌入到索引文档中，以减少查询时的联表操作和数据读取次数，从而提升查询性能。**Lucene 中，执行类似于关系型数据库的联表操作是不现实的，因为 Lucene 是一个基于倒排索引的搜索引擎，主要优化的是全文搜索性能，而不是关系查询**。

>  注意：在关系型数据库中，反范式化的数据更新需要更加小心，以确保数据一致性。在Lucene中，索引的更新可能更加复杂，因为索引文档需要重新构建。

而Lucene面临的一个挑战是解决有关文档真实结构和Lucene表示能力之间的“不匹配”问题。举例来说，XML文档通过嵌套标记来表示一个递归的文档结构，数据库可能有任意数量的连接点，表之间可以通过主键和次键相互关联起来。微软的ObjectLinking&Embedding（OLE）文档可以指向其他嵌入类文档。然而Lucene的文档却都是单一文档，因此在创建对应的Lucene文档前，必须对上述递归文档结构和连接点进行反向规格化操作。建立在Lucene基础之上的开源项目，如HibemateSearch、Compass、LuSQL、DBSight、Browse Engine和Oracle/Luceneintergration等，都有各自不同而有趣的方法来解决反向规格化问题。


### 理解索引过程

lucene的索引过程如图：

<div style={{ textAlign: 'center', width: '100%' }}>
    <img src="/analysis-project/img/lucene/lucene-2.png" alt="image" style={{ maxWidth: '40%', height: 'auto', display: 'block', margin: '0 auto' }} />
</div>

[//]: # (<img src="/Users/liufei/docs/typora笔记/images/lucene/lucene-2.png" alt="img" style="zoom:40%;" />)



**提取文档(extract text)**

这一步使用`tika`等工具可以完成，将数据转化为文档和域

**分析文档(analysis)**

一旦建立起Lucene 文档和域，就可以调用 IndexNriter 对象的addDocument 方法将数据传递给Lucene 进行索引操作了。在索引操作时，Lucene 首先分析文本，将文本数据分割成语汇单元串，然后对它们执行一些可选操作。例如，语汇单元在索引前需要统一转换力小写，以使搜索不对大小写敏感，这个操作可以通过调用 Lucene 的 LowerCaseFilter 类实现。通常还需要调用StopEi1ter 类从输入中去掉一些使用很频繁却没有实际意义的词（如a、an、the、in、on、so on 等英文文本）。同样地，我们也需要分析输入的语汇单元，去掉它们的词干，如调用 PorterstemFi1ter 类处理英文文本（对于其他语种，Lucene 的contrib 分析模块有对应的类调用）。这些将原始数据转换语汇单元，随后用一系列 filter来修正该语汇单元的操作，一起构成了分析器。另外你还可以通过链接Lucene 的语汇单元和 filter 来搭建自己的分析器，或者通过其他自定义方式来搭建分析器。

以上称之为分析的步骤很重要，分析过程会产生大批的语汇单元，随后这些语汇单元将被写人索引文件中。

**将文档索引**

对输入数据分析完毕后，就可以将分析结果写入索引文件中。Lucene 将输入数据以一种倒排索引 （inverted index）的数据结构进行存储。在进行关键字快速查找时，这种数据结构能够有效利用磁盘空间。Lucene 使用倒排数据结构的原因是：把文档中提取出的语汇单元作为查询关键字，而不是将文档作为中心实体，这种思想很像本书索引与页码的对应关系。换句话说，倒排索引并不是回答“这个文档中包含哪些单词？”这个问题，而是经过优化后用来快速回答“哪些文档包含单词 X？”这个问题。

**索引文件**

lucene索引文件是以段`segement_x`来存储的，每个段都是一个独立的索引，它包含整个文档索引的一个子集。每当 writer 刷新缓冲区增加的文档，以及挂起目录删除操作时，索引文件都会建立一个新段。在搜索索引时，每个段都是单独访问的，但搜索结果是合并后返回的。

每个段都包含多个文件，文件格式为`_X.<ext>`，这里×代表段名称，`<ext>`为扩展名，用来标识该文件对应索引的某个部分。各个独立的文件共同组成了索引的不同部分（项向量、存储的域、倒排索引，等等）。如果使用混合文件格式*（这是 Lucene 默认处理方式，但可以通过 IndexWriter.setUsecompoundFile 方法进行修改）*，那么上述索引文件都会被压缩成一个单一的文件：`X.cfs`。这种方式能在搜索期间减少打开的文件数量。还有一个特殊文件，名叫段文件，用段`<N>`标识，该文件指向所有激活的段。段文件非常重要！Lucene 会首先打开该文件，然后打开它所指向的其他文件。值`<N>`被称为 “the generation”，它是一个整数，Lucene 每次向索引提交更改时都会将这个数加1。久而久之，索引会积聚很多段，特别是当程序打开和关闭writer 较为频繁时。这种情况是没问题的。`IndexWriter` 类会周期性地选择一些段，然后将它们合并到一个新段中，然后删除老的段。被合并段的选取策略由一个独立的`MergePolicy`类主导。一旦选取好这些段，具体合并操作由 `Mergescheduler` 类实现。



### 索引操作

lucene的新增和删除都还好，lucene的文档更新策略需要单独说一下：Lucene 只能删除整个旧文档，然后向索引中添加新文档。这要求新文档必须包含旧文档中所有域，包括内容未发生改变的域。**注意lucene包括elasticsearch的更新方法都是通过调用 deleteDocuments 和 addDocument 两个方法合并实现的**，所以更新时必须小心更新的条件和内容，条件必须要准确定位到要更新的文档，否则条件错误将删除别的数据，内容则必须将已有的内容也要包括，否则将丢失已有的数据。

>  更新的内部过程：标记旧文档为删除（但不会立即物理删除）-->  新增一个包含更新后数据的新文档 --> 使用乐观并发控制（版本号）来确保更新的一致性。

虽然这样的更新策略会占用更多的磁盘空间，但却有如下的好处，这也就为什么要采取这种更新策略的原因：

- **简单高效**：通过先删除再新增，避免了复杂的文档修改操作，使得索引操作更为简单高效。

- **并发控制**：版本号机制确保了并发更新时的数据一致性，避免了数据冲突。

- **优化搜索性能**：分段合并过程中会进行优化，删除无效文档，压缩索引，提高搜索性能。



**Elasticsearch的更新时需要注意的细节**

- 精确定位文档：每个文档在索引中都有一个唯一的标识符（ID）。在执行更新操作时，必须提供这个唯一标识符，以便 Elasticsearch 能够准确定位到要更新的文档。

- 更新内容的保护：通过 `_update` API，使用 `doc` 参数只更新文档的指定字段，而其他字段保持不变。

- 部分更新：`_update` API “部分更新”的机制背后的实现仍然是通过删除旧文档并新增一个包含更新后数据的新文档来完成的。关键区别在于，es 会自动合并新旧数据，以确保未更新的字段保持不变。

```json
POST /my_index/_update/1
{
  "doc": {
    "title": "Updated Title"
  }
}
    ```

- 全量更新：此put方法全量更新会**覆盖已有的整个文档，会丢失数据**

```json
PUT /my_index/_doc/1
{
  "title": "Updated Title"
}
    ```



### Field选项

Field类也许是在文档索引期间最重要的类了：该类在事实上控制着被索引的域值。

先讲一个搜索模型，这涉及到lucene的为了处理搜索而做的索引操作。

#### 常见的三种搜索理论模型

- 纯布尔模型（Pure Boolean model）：文档不管是否匹配查询请求，都不会被评分。在该模型下，匹配文档与评分不相关，也是无序的；一条查询仅获取所有匹配文档集合的一个子集。
- 向量空间模型（Vector space model）：查询语句和文档都是高维空间的向量模型，这里每一个独立的项都是一个维度。查询语句和文档之间的相关性或相似性由各自向量之间的距离计算得到。
- 概率模型（Probabilistic model）：在该模型中，采用全概率方法来计算文档和查询语句的匹配概率。



#### Field索引选项

Field索引选项（Field.Index.*）通过倒排索引来控制域文本是否可被搜索。具体选项如下：

- `Index.ANALYZED`-使用分析器将域值分解成独立的语汇单元流，并使每个语汇单元能被搜索。该选项适用于普通文本域（如正文、标题、摘要等）。

- `Index. NOT_ANALYZED`对域进行索引，但不对 String 值进行分析。该操作实际上将域值作为单一语汇单元并使之能被搜索。该选项适用于索引那些不能被分解的域值，如URL、文件路径、日期、人名、社保号码和电话号码等。该选项尤其适用于“精确匹配”搜索。在程序2.1 和程序2.3中我们曾使用这个选项索引 ID域。

- `Index. ANALYZED_NO _NORMS`: 这是 `Index.ANALYZED` 选项的一个变体，它不会在索引中存储norms 信息。norms 记录了索引中的 index-time boost 信息，但是当你进行搜索时可能会比较耗费内存。有关norms 的详细内容详见2.5.3小节。

- `Index.NOT ANALYZED_NO_NORMS`: 与 `Index.NOT_ANALYZED` 选项类似，但也是不存储 norms。该选项常用于在搜索期间节省索引空间和减少内存耗费，因为 single-token 域并不需要norms 信息，除非它们已被进行加权操作。

- `Index.NO`：使对应的域值不被搜索。

*Field.Index.\*中，除了Field.Index.NO，其他都是会被索引的，Field.Index.NO字段的值不会被存储在倒排索引中，因此不能通过搜索来检索该字段的内容。*

> norm（规范化信息）的概念：在 Lucene 和 Elasticsearch 中的主要作用之一是用于处理搜索相关性评分模型，特别是在使用向量空间模型（Vector Space Model，VSM）时非常重要。记录的信息比如：长度、字段权重等。
>
> **长度规范化**（Field Length Normalization）：
>
> - Norms 包含有关文档中每个字段的长度信息。在搜索时，这些信息用于归一化每个文档向量的长度。
> - 归一化文档长度是为了处理不同长度文档的相关性评分。**较短的文档可能因其内容更为紧凑而被认为更相关**。
>
> **字段权重**（Boost Factor）：
>
> - Norms 也包含字段的权重信息。权重反映了字段在整个索引中的相对重要性。
> - 在搜索时，字段的权重可以影响其在向量空间中的权重，从而影响文档与查询的相似度计算。
>
> 上面的Field.Index.*配置，其实对应es中就是mapping中的字段配置，比如`Index.NOT ANALYZED_NO_NORMS`对于的配置就是：
>
> ```json
> PUT /my_index
> {
>   "mappings": {
>     "properties": {
>       "content": {
>         "type": "text",
>         "analyzer": "standard",
>         "norms": false
>       }
>     }
>   }
> }
> ```



当Lucene 建立起倒排索引后，默认情况下它会保存所有必要信息以实施 Vector SpaceModel。该Model 需要计算文档中出现的term 数，以及它们出现的位置（这是必要的，比如通过词组搜索时用到）。但有时候这些域只是在布尔搜索时用到，它们并不相关评分做贡献，一个常见的例子是，域只是被用作过滤，如权限过滤和日期过滤。在这种情况下，可以通过调用 Field.setomitTermFreqAndPositions （true）方法让Lucene 跳过对该项的出现频率和出现位置的索引。该方法可以节省一些索引在磁盘上的存储空间，还可以加速搜索和过滤过程，但会悄悄地阻止需要位置信息的搜索，如阻止 PhraseQuery 和Spanguery类的运行。

#### Field存储选项

域存储选项 （Field.Store.*）用来确定是否需要存储域的真实值，以便后续搜索时能恢复这个值。

`Store.YES`—指定存储域值。该情况下，原始的字符串值全部被保存在索引中，并可以由 IndexReader类恢复。该选项对于需要展示搜索结果的一些域很有用（如 URL、标题或数据库主键）。如果索引的大小在搜索程序考虑之列的话，不要存储太大的域值，因为存储这些域值会消耗掉索引的存储空间。

`Store.NO`—指定不存储域值。该选项通常跟 Index.ANALYZED选项共同用来索引大的文本域值，通常这些域值不用恢复为初始格式，如 Web 页面的正文，或其他类型的文本文档。

> 在 Elasticsearch 中，与 Lucene 中的 `Field.Store.YES` 相对应的概念是文档字段是否被存储（stored）在 `_source` 字段中，每个文档都有一个特殊的 `_source` 字段，用于存储文档的原始 JSON 内容。这个字段默认情况下是启用的，它包含了整个文档的内容，包括所有存储在文档中的字段。可以在映射（mappings）中定义字段的存储行为。通过禁用 `_source` 来节省存储空间，但这样会使得文档无法完整地返回给客户端。
>
> ```json
> PUT /my_index
> {
> "mappings": {
>  "properties": {
>    "content": {
>      "type": "text",
>      "store": true
>    }
>  }
> }
> }
> ```

并且对应存储域的值还能进行压缩，可以为索引节省一些空间，但节省的幅度跟域值的可被压缩程度有关，而且该方法会降低索引和搜索速度。这样其实就是通过消耗更多CPU 计算能力来换取更多的磁盘空间，对于很多程序来说，需要仔细权衡一下。如果域值所占空间很小，建议少使用压缩。



#### Field项向量

项向量（Term Vector）是一个用于存储文档中词项（term）信息的数据结构。它包括了文档中出现的每个词项及其相关的信息，如词项的位置、频率等。项向量用于加速查询、提高索引效率以及支持高级搜索功能。

**项向量的组成**：

- 词项列表：包含文档中出现的所有词项。
- 词项频率（Term Frequency）：记录每个词项在文档中出现的次数。
- 词项位置（Term Position）：记录每个词项在文档中的具体位置（可以是多个位置）。
- 词项偏移量（Term Offset）：记录词项在文档中字符级别的开始和结束位置。

**使用项向量的目的**：

- 快速高效的查询：项向量允许快速访问文档中包含的词项信息，这可以加速某些查询操作，如短语查询、近邻查询等。
- 支持高级搜索功能：例如，高亮显示查询匹配的词、计算相似度等。
- 文本分析和自然语言处理（NLP）：项向量为文档的内容分析和处理提供了详细的词项信息，这对于实现复杂的文本处理算法非常有用。

**项向量的选项**

- no：不存储项向量（默认值）。
- yes：仅存储词项列表和词项频率。
- with_positions：存储词项列表、词项频率以及词项位置。
- with_offsets：存储词项列表、词项频率以及词项偏移量。
- with_positions_offsets：存储词项列表、词项频率、词项位置以及词项偏移量。

```json
# mapping配置例子
PUT /my_index
{
  "mappings": {
    "properties": {
      "text_field": {
        "type": "text",
        "term_vector": "with_positions_offsets"
      }
    }
  }
}
```

>文档："The quick brown fox jumps over the lazy dog."
>
>启用项向量后，Lucene会为该文档存储类似如下的信息：
>
>- 词项列表：\["The", "quick", "brown", "fox", "jumps", "over", "the", "lazy", "dog"\]
>- 词项频率：\{"The": 1, "quick": 1, "brown": 1, "fox": 1, "jumps": 1, "over": 1, "the": 1, "lazy": 1, "dog": 1\}
>- 词项位置：\{"The": \[0\], "quick": \[1\], "brown": \[2\], "fox": \[3\], "jumps": \[4\], "over": \[5\], "the": \[6\], "lazy": \[7\], "dog": \[8\]\}
>- 词项偏移量：\{"The": \[(0, 3)\], "quick": \[(4, 9)\], "brown": \[(10, 15)\], "fox": \[(16, 19)\], "jumps": \[(20, 25)\], "over": \[(26, 30)\], "the": \[(31, 34)\], "lazy": \[(35, 39)\], "dog": \[(40, 43)\]\}



**Field选项组合**

| 索引选项              | 存储选项 | 项向量                 | 使用范例                                                     |
| --------------------- | -------- | ---------------------- | ------------------------------------------------------------ |
| NOT ANALYZED NO NORMS | YES      | NO                     | 标识符（文件名、主键），电话号码和社会安全号码、URL、姓名、日期、用于排序的文本域 |
| ANALYZED              | YES      | WITH_POSITIONS_OFFSETS | 文档标题、摘要                                               |
| ANALYZED              | NO       | WITH POSITIONS OFFSETS | 文档正文                                                     |
| NO                    | YES      | NO                     | 文档类型、数据库主键（如果没有用于搜索）                     |
| NOT ANALYZED          | NO       | NO                     | 隐藏的关键词                                                 |



#### Field排序

当 Lucene 返回匹配搜索条件的文档时，一般是按照默认评分对文档进行排序的。有时你可能需要依照其他标准对结果进行排序，比如在搜索E-mail信息时，你可能会根据发送或接收日期排序，或者根据信息大小或寄件人排序。为了实现域排序功能，你必须首先正确地完成对域的索引。如果域是数值类型的，在将它加入文档和进行排序时，要用 NumericField类来表示。如果域是文本类型的，如邮件发送者姓名，你得用Field类来表示它和索引它，并且要用 Field.Index.NOT_ANALYZED 选项避免对它进行分析。如果你的域未进行加权操作，那么在对其索引时就不能带有 norm 选项。



#### Field处理集合

在lucene中是支持多值的，即类似java中的`List<T>`，es中字段mapping也只需要配置T的类型即可，自动支持集合。

比如：author字段有多个，则可以直接如下处理

```java
Document doc = new Document ();
for (String author : authors) {
    doc.add (new Field ("author", author,Field. Store. YES,Field. Index. ANALYZED) );
｝
```

这种处理方式lucene中是完全可以接受并鼓励使用的，因为这是逻辑上具有多个域值的域的自然的表示方式。在程序内部，只要文档中出现同名的多值域，倒排索引和项向量都会在逻辑上将这些域的语汇单元附加进去，具体顺序由添加该域的顺序决定。你也可以在分析期间使用高级选项来控制有关附加顺序的重要细节，特别是如何防止针对两个不同域值的匹配搜索。然而与索引操作不同的是，当存储这些域时，它们在文档中的存储顺序是分离的，因此当你在搜索期间对文档进行检索时，你会发现多个Field实例。也就是说在集合中搜索单个值也能正常匹配出结果的。

